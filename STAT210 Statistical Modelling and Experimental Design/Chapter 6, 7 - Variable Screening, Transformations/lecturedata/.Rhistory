results <- cbind(improve, Px, PLTx, PGTx)
dimnames(results) <- list(NULL, c("x", "P(X = x)", "P(X <= x)", "P(X > x)"))
results
head(results, 10)
lambda = 2
p = ppois(5-1, lambda=lambda, lower.tail=FALSE)
1 - ppois(4, lambda=lambda)
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
customers <- rpois(n = 100, lambda=lambda)
customers
mean(customers)
customers[10]
hist(customers)
p.sim <- sum(customers >= 5) / 100; p.sim
p
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = 100, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / 100; p.sim
p
sim.results <- table(customers)
sim.results <- table(customers); sim.results
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = 100, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / 100; p.sim
p
sim.results <- table(customers); sim.results
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = 100, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / 100; p.sim
p
sim.results <- table(customers); sim.results
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = 100, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / 100; p.sim
p
sim.results <- table(customers); sim.results
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = 100, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / 100; p.sim
p
sim.results <- table(customers); sim.results
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = 100, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / 100; p.sim
p
sim.results <- table(customers); sim.results
1/6
2/7
sum(sim.results >= 5)
data.frame(PSim=p.sim, P=p)
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = 100, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / 100; p.sim
p
# There are 3 intervals here where at least 5 customers entered
sim.results <- table(customers); sim.results
sum(sim.results >= 5)
data.frame(PSim=p.sim, P=p)
sum(c(13, 19, 32, 17, 10,  5,  2,  2))
N <- 1000
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = N, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / N; p.sim
p
# There are 3 intervals here where at least 5 customers entered
sim.results <- table(customers); sim.results
data.frame(PSim=p.sim, P=p)
# Unlikely P(X >= 5) can suspect that mu = 2 may be greater than 2
# if indeed we think five or more customers arrive in a half minute interval
# in our observation.
N <- 1000
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = N, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / N; p.sim
p
# There are 3 intervals here where at least 5 customers entered
sim.results <- table(customers); sim.results
data.frame(PSim=p.sim, P=p)
# Unlikely P(X >= 5) can suspect that mu = 2 may be greater than 2
# if indeed we think five or more customers arrive in a half minute interval
# in our observation.
w <- 1:3
rbind(w, pmf.W(w))
# Question 1
pmf.W <- function(w) {
5*w / (6 * (1 + w^2))
}
# a) is valid PDF since it sums to 1
sum(pmf.W(1) + pmf.W(2) + pmf.W(3))
# and all values are positive
all(c(pmf.W(1) > 0, pmf.W(2) > 0, pmf.W(3) > 0))
# another way to check all values are positive
w <- 1:3
rbind(w, pmf.W(w))
# b) find E(W)
mu.W <- pmf.W(1) * 1 + pmf.W(2) * 2 + pmf.W(3) * 3
mu.W
# c) var(W)
# E(W^2)
mu2.W <- pmf.W(1) * 1^2 + pmf.W(2) * 2^2 + pmf.W(3) * 3^3
mu2.W
# V(W)
var.w <- mu2.W - mu.W^2
var.w
# d) TODO why not the same as above?
var.another.W <- pmf.W(1)*(1-mu.W)^2 + pmf.W(2)*(2-mu.W)^2 + pmf.W(3)*(3-mu.W)^2
var.another.W
# NUMBER 2 Integration
f.X <- function(x) { 2*(1 - x) }
# a)
integrate(f.X, 0, 1) # so is valid prob func
# b)
f.ex <- function(x) {2*x*(1-x)}
EX <- integrate(f.ex, 0, 1)
EX
names(EX)
EX$value
# c)
f.var <- function(x) { (x - EX$value)^2 * 2 * (1 - x)}
VAR.X <- integrate(f.var, 0, 1)
VAR.X$value
# NUMBER 3 Dice
# a)
diceSample <- sample(1:6, size=120, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
table(diceSample)/120
# b)
mean(diceSample) # true = 3.5
sqrt(var(diceSample)) # true = 1.708
# c)
barplot(table(diceSample))
hist(diceSample)
library(ggplot2)
df <- data.frame(diceSample)
ggplot(df, aes(diceSample)) + geom_histogram(binwidth=1, fill='dodgerblue')
rbind(w, pmf.W(w))
pmf.W(w)
sum(pmf.W(w))
mu.W <- pmf.W(1) * 1 + pmf.W(2) * 2 + pmf.W(3) * 3
mu.W
w*pmf.W(w)
sum(w*pmf.W(w))
sum(w^2 * pmf.W(w)) - mu.W^2
var.w <- mu2.W - mu.W^2
var.w
var.W <- sum(w^2 * pmf.W(w)) - mu.W^2
var.W
var.W <- sum(w^2 * pmf.W(w)) - mu.W^2
var.W
var.another.W <- pmf.W(1)*(1-mu.W)^2 + pmf.W(2)*(2-mu.W)^2 + pmf.W(3)*(3-mu.W)^2
var.another.W
f.X <- function(x) { 2*(1 - x) }
# a)
integrate(f.X, 0, 1) # so is valid prob func
f.ex <- function(x) {2*x*(1-x)}
EX <- integrate(f.ex, 0, 1)
EX
names(EX)
EX$value
# c)
f.var <- function(x) { (x - EX$value)^2 * 2 * (1 - x)}
VAR.X <- integrate(f.var, 0, 1)
VAR.X$value
diceSample <- sample(1:6, size=120, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
table(diceSample)/120
# b)
mean(diceSample) # true = 3.5
sqrt(var(diceSample)) # true = 1.708
sd(diceSample)
barplot(table(diceSample))
dice1 <- sample(1:6, size=10^4, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice2 <- sample(1:6, size=10^4, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice3 <- sample(1:6, size=10^4, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice1
diceSum <- dice1 + dice2 + dice3
diceSum
N <- 10^4
probLessTen <- sum(diceSum < 10) / N; probLessTen
head(dice1, dice2, dice3)
head(cbind(dice1, dice2, dice3))
df <- cbind(dice1, dice2, dice3)
df[6,]
unique(df[6,])
df[6,1] == df[6,2] == df[6,3]
df[6,1] == df[6,2] || df[6,1] == df[6,3] || df[6,2] == df[6,3]
rolls <- cbind(dice1, dice2, dice3)
c = -
c = 0
c = 0
c += 1
probFaceValuesAllDifferent
probFaceValuesAllDifferent <- count / N;
probFaceValuesAllDifferent
probFaceValuesAllDifferent <- count / N;
rolls <- cbind(dice1, dice2, dice3)
#unique(df[6,])
count <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count = count + 1
}
}
# the probability we are finding is:
probFaceValuesAllDifferent <- count / N;
probFaceValuesAllDifferent
count <- 0
unique(rolls)
head(unique(rolls))
head(rolls)
count <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(unique(rolls[i, ]) == 3){
# then we increment the count
count = count + 1
}
}
count /
count / N
count <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(unique(rolls[i, ]) == 3){
# then we increment the count
count = count + 1
}
}
count / N
count <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count = count + 1
}
}
count
count <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(unique(rolls[i, ]) == 3){
# then we increment the count
count = count + 1
}
}
warnings()
count <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(length(unique(rolls[i, ])) == 3){
# then we increment the count
count = count + 1
}
}
count
count.1 <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count.1 = count.1 + 1
}
}
# the probability we are finding is:
probFaceValuesAllDifferent.1 <- count.1 / N;
probFaceValuesAllDifferen.1
probFaceValuesAllDifferent.1
count.2 <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(length(unique(rolls[i, ])) == 3){
# then we increment the count
count.2 = count.2 + 1
}
}
probFaceValuesAllDifferent.2 <- count.2 / N
assertthat::count.1 == count.2
assertthat(count.1 == count.2)
assert_that(count.1 == count.2)
count.1 == count.2
probFaceValuesAllDifferent.2
1 - pbinom(100, size=105, n = 0.10)
1 - pbinom(100, size=105, prob = 0.10)
pbinom(100, size = 105, prob = 0.10, lower.tail=FALSE)
1 - pbinom(100, size=105, prob = 0.90)
pbinom(100, size = 105, prob = 0.90, lower.tail=FALSE)
ppois(11, lambda=10, lower.tail = FALSE)
1 - ppois(11, lambda=10)
p = ppois(11, lambda=10, lower.tail = FALSE)
pbinom(2, size=8, prob=p, lower.tail = FALSE)
1 - pbinom(2, size=8, prob=p)
p = pgeom(2, prob=0.8, lower.tail = FALSE)
p = pgeom(2, prob=0.8, lower.tail = FALSE); p
p = pgeom(q=2, prob=0.8, lower.tail = FALSE); p
1 - pgeom(2, prob=0.8)
p = pgeom(q=3, prob=0.8, lower.tail = FALSE); p
p = pgeom(q=1, prob=0.8, lower.tail = FALSE); p
?pgeom
p = pgeom(q=2, prob=0.8, lower.tail = FALSE); p
p = pgeom(q=1, prob=0.8, lower.tail = FALSE); p
1 - pgeom(1, prob=0.8)
pbinom(3, size=10, prob = p, lower.tail=FALSE)
1 - pbinom(3, size=10, prob=p)
N <- 10^4
dice1 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice2 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice3 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
diceSum <- dice1 + dice2 + dice3
# part a) Probability that sum of face values < 10
probLessTen <- sum(diceSum < 10) / N; probLessTen
rolls <- cbind(dice1, dice2, dice3)
count.1 <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count.1 = count.1 + 1
}
}
# the probability we are finding is:
probFaceValuesAllDifferent.1 <- count.1 / N;
probFaceValuesAllDifferent.1
count.2 <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(length(unique(rolls[i, ])) == 3){
# then we increment the count
count.2 = count.2 + 1
}
}
probFaceValuesAllDifferent.2 <- count.2 / N
probFaceValuesAllDifferent.2
# Test to make sure: Should be true, the two methods yield the same answer.
count.1 == count.2
1 - pbinom(100, size=105, prob = 0.90)
# method 2 of calculating
pbinom(100, size = 105, prob = 0.90, lower.tail=FALSE)
p = ppois(11, lambda=10, lower.tail = FALSE)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/Assignment2_questions_1_4_5.R', echo=TRUE)
p = ppois(11, lambda=10, lower.tail = FALSE); p
1 - ppois(11, lambda=10)
pbinom(100, size = 105, prob = 0.90, lower.tail=FALSE)
p = ppois(11, lambda=10, lower.tail = FALSE); p
# method 2
1 - ppois(11, lambda=10)
pbinom(2, size=8, prob=p, lower.tail = FALSE)
# method 2
1 - pbinom(2, size=8, prob=p)
p = pgeom(q=1, prob=0.8, lower.tail = FALSE); p
# Method 2
1 - pgeom(1, prob=0.8)
pbinom(3, size=10, prob = p, lower.tail=FALSE)
# Method 2
1 - pbinom(3, size=10, prob=p)
p = pgeom(q=0, prob=0.8, lower.tail = FALSE); p
1 - pgeom(1, prob=0.8) # P(X >= 2) = P(X <= 1)
1 - pgeom(2, prob=0.8) # P(X >= 2) = P(X <= 1)
1 - pgeom(0, prob=0.8) # P(X >= 2) = P(X <= 1)
pbinom(3, size=10, prob = p, lower.tail=FALSE)
1 - pbinom(3, size=10, prob=p)
library(ggplot2)
library(ggfortify)
model <- glm(mpg ~ wt, data = mtcars, family = gaussian())
cd_cont_pos <- function(leverage, level, model) {sqrt(level*length(coef(model))*(1-leverage)/leverage)}
cd_cont_neg <- function(leverage, level, model) {-cd_cont_pos(leverage, level, model)}
autoplot(model, which = 5) +
stat_function(fun = cd_cont_pos, args = list(level = 0.5, model = model), xlim = c(0, 0.25), lty = 2, colour = "red") +
stat_function(fun = cd_cont_neg, args = list(level = 0.5, model = model), xlim = c(0, 0.25), lty = 2, colour = "red") + scale_y_continuous(limits = c(-2, 2.5))
library(MASS)
data(Cars93)
cars_lm <- lm(Price ~ Passengers + Length + RPM, data = Cars93)
gg_boxcox(cars_lm)
install.packages("lindia")
plotInfluence <- function (model, fill="white",
outline="black", size=30) {
require(ggplot2)
if(!inherits(model, "lm"))
stop("You need to supply an lm object.")
df<-data.frame(Residual=rstudent(model),
Leverage=hatvalues(model),
Cooks=cooks.distance(model),
Observation=names(hatvalues(model)),
stringsAsFactors=FALSE)
myxint<-c(2*mean(df$Leverage), 3*mean(df$Leverage))
inds<-intersect(which(abs(df$Residual) < 2),
which( df$Leverage < myxint[1]))
if(length(inds) > 0) df$Observation[inds]<-""
ggplot(df, aes_string(x='Leverage', y='Residual',
size='Cooks', label='Observation'),
legend=FALSE) +
geom_point(colour=outline, fill=fill, shape=21) +
scale_size_area(max_size=size) +
theme_bw(base_size=16) + geom_text(size=4) +
geom_hline(yintercept=c(2,-2), linetype="dashed") +
geom_vline(xintercept=myxint, linetype="dashed") +
ylab("Studentized Residuals") +
xlab("Hat-Values") + labs(size="Cook's distance")
}
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 6, 7 - Variable Screening, Transformations/Lecture_Chapter7_Influential_LifeCycle.R', echo=TRUE)
plotInfluence(sr.lm)
plotInfluence <- function (model, size=30) {
require(ggplot2)
if(!inherits(model, "lm"))
stop("You need to supply an lm object.")
df<-data.frame(Residual=rstudent(model),
Leverage=hatvalues(model),
Cooks=cooks.distance(model),
Observation=names(hatvalues(model)),
stringsAsFactors=FALSE)
myxint<-c(2*mean(df$Leverage), 3*mean(df$Leverage))
inds<-intersect(which(abs(df$Residual) < 2),
which( df$Leverage < myxint[1]))
if(length(inds) > 0) df$Observation[inds]<-""
ggplot(df, aes_string(x='Leverage', y='Residual',
size='Cooks', label='Observation'),
legend=FALSE) +
geom_point(colour="black", shape=19) +
scale_size_area(max_size=size) +
geom_hline(yintercept=c(2,-2), linetype="dashed") +
geom_vline(xintercept=myxint, linetype="dashed") +
ylab("Studentized Residuals") +
xlab("Hat-Values") + labs(size="Cook's distance")
}
plotInfluence(sr.lm)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 8 - Residuals/Lecture_Chapter8_Influential_LifeCycle.R', echo=TRUE)
q()
