sqrt(var(diceSample)) # true = 1.708
# c)
barplot(table(diceSample))
hist(diceSample)
library(ggplot2)
df <- data.frame(diceSample)
ggplot(df, aes(diceSample)) + geom_histogram(binwidth=1, fill='dodgerblue')
diceSample <- sample(1:6, size=120, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
table(diceSample)/120
# b)
mean(diceSample) # true = 3.5
sqrt(var(diceSample)) # true = 1.708
# c)
barplot(table(diceSample))
hist(diceSample)
library(ggplot2)
df <- data.frame(diceSample)
ggplot(df, aes(diceSample)) + geom_histogram(binwidth=1, fill='dodgerblue')
diceSample <- sample(1:6, size=120, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
table(diceSample)/120
# b)
mean(diceSample) # true = 3.5
sqrt(var(diceSample)) # true = 1.708
# c)
barplot(table(diceSample))
hist(diceSample)
library(ggplot2)
df <- data.frame(diceSample)
ggplot(df, aes(diceSample)) + geom_histogram(binwidth=1, fill='dodgerblue')
diceSample <- sample(1:6, size=120, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
table(diceSample)/120
# b)
mean(diceSample) # true = 3.5
sqrt(var(diceSample)) # true = 1.708
# c)
barplot(table(diceSample))
hist(diceSample)
library(ggplot2)
df <- data.frame(diceSample)
ggplot(df, aes(diceSample)) + geom_histogram(binwidth=1, fill='dodgerblue')
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/Module 4_5 - Discrete and Continuous Distributions/ProbabilityDistributions.R', echo=TRUE)
par(mfrow=c(1,1))
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/Module 4_5 - Discrete and Continuous Distributions/ProbabilityDistributions.R', echo=TRUE)
par (mfrow = c(1,1))
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/Module 4_5 - Discrete and Continuous Distributions/ProbabilityDistributions.R', echo=TRUE)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/")
load("data/Exercises and Examples/DIESEL.Rdata")
load("data/Exercises and Examples/DIESEL2.Rdata")
load("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/data/Exercises and Examples/DIESEL2.Rdata")
load("data/Exercises and Examples/DIELSEL2.Rdata")
library(ggplot2)
options(digits=10, show.signif.stars = FALSE)
diesel.lm <- lm(PERFORM ~ BRAND + FUEL, data=DIESEL)
summary(diesel.lm)
# PLOTTING INTERACTION
attach(DIESEL)
# we can see fthat for different levels of Fuel (F1, F2, F3), there
# are non-parallel brand curves of performance. Interaction!
# METHOD 1
ggplot(data=DIESEL, aes(x=BRAND, y=PERFORM, group=FUEL, color=FUEL)) +
geom_smooth(method=lm, lwd=1, se=FALSE) +
ggtitle("Interaction Plot of Brand vs. Performance for \n
Different Levels of Fuel")
# METHOD 2
interaction.plot(x.factor = BRAND, trace.factor = FUEL, response = PERFORM)
args(interaction.plot)
detach(DIESEL)
# === Now for when data is missing - when any data is missing, ALL
# interaction term coefficients are missing!
# No data for F3B1 and F1B2
attach(DIESEL2)
attach(DIELSEL2)
diesel.MISSING.lm <- lm(PERFORM ~ BRAND + FUEL, data=DIELSEL2)
summary(diesel.MISSING.lm)
summary(diesel.lm)
View(DIELSEL2)
lm(PERFORM ~ X1 + X2 + X3 + X1X2 + X2X3)
lm(PERFORM ~ X1 + X2 + X3 + X1X3 + X2X3)
diesel.MISSING.lm <- lm(PERFORM ~ BRAND + FUEL, data=DIELSEL2)
summary(diesel.MISSING.lm)
lm(PERFORM ~ X1 + X2 + X3 + X1X3 + X2X3)
View(DIESEL)
diesel.MISSING.butWorks.lm <- lm(PERFORM ~ BRAND + FUEL, data=DIELSEL2)
summary(diesel.MISSING.butWorks.lm)
diesel.MISSING.lm <- lm(PERFORM ~ X1 + X2 + X3 + X1X3 + X2X3)
summary(diesel.MISSING.lm)
diesel.MISSING.butWorks.lm <- lm(PERFORM ~ BRAND + FUEL + FUELD:BRAND, data=DIELSEL2)
diesel.MISSING.butWorks.lm <- lm(PERFORM ~ BRAND + FUEL + FUEL:BRAND, data=DIELSEL2)
summary(diesel.MISSING.butWorks.lm)
summary(diesel.MISSING.lm)
diesel.MISSING.butWorks.lm <- lm(PERFORM ~ FUEL + BRAND + FUEL:BRAND, data=DIELSEL2)
summary(diesel.MISSING.butWorks.lm)
diesel.MISSING.XS.lm <-lm(PERFORM ~ X1 + X2 + X3 + X1X3 + X2X3, data=DIELSEL2)
summary(diesel.MISSING.XS.lm)
diesel.MISSING.lm <- lm(PERFORM ~ FUEL + BRAND + FUEL:BRAND, data=DIELSEL2)
summary(diesel.MISSING.XS.lm)
summary(diesel.MISSING.lm)
summary(diesel.lm)
ggplot(data=DIELSEL2, aes(x=BRAND, y=PERFORM, group=FUEL, color=FUEL)) +
geom_smooth(method=lm, lwd=1, se=FALSE) +
ggtitle("Interaction Plot of Brand vs. Performance for \n
Different Levels of Fuel")
detach(DIESEL)
interaction.plot(x.factor = BRAND, trace.factor = FUEL, response = PERFORM)
detach(DIELSEL2)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/")
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/FORMULAS.R', echo=FALSE)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/INTERPRET.R', echo=FALSE)
load("data/Exercises and Examples/GFCLOCKS.Rdata")
clock.interact.lm <- lm(PRICE ~ AGE + NUMBIDS + AGE_BID, data=GFCLOCKS)
summary(clock.interact.lm)
ggplot(GFCLOCKS, aes(x=AGE, y=PRICE, group = NUMBIDS, color=NUMBIDS)) +
geom_smooth(method="lm", se=FALSE) +
ggtitle("Interaction plot of Age With Price For Different
Levels of Bidders")
ggplot(GFCLOCKS, aes(x=AGE, y=PRICE, group = NUMBIDS, color=NUMBIDS)) +
geom_point() +
geom_smooth(method="lm", se=FALSE) +
ggtitle("Interaction plot of Age With Price For Different
Levels of Bidders")
ggplot(GFCLOCKS, aes(x=AGE, y=PRICE, group = factor(NUMBIDS),
color=factor(NUMBIDS))) +
geom_point() +
geom_smooth(method="lm", se=FALSE) +
ggtitle("Interaction plot of Age With Price For Different
Levels of Bidders")
library(car) #Even though we already installed "car", we have to tell R we want it to load this package for us to use
#You can choose whatever # you want for the seed; this is for randomization of your data set
set.seed(150)
#Let's make our data set will have 250 participants (n), perhaps college students!
n <- 250
#Uniform distribution of work ethic (X) from 1-5 (1 = poor work ethic, 5 = great work ethic)
X <- rnorm(n, 2.75, .75)
#We want a normal distribution of IQ (Z)
#I fixed the mean of IQ to 15 so that the regression equation works realistically, SD = 15
Z <- rnorm(n, 15, 15)
#We then create Y using a regression equation (adding a bit of random noise)
Y <- .7*X + .3*Z + 2.5*X*Z + rnorm(n, sd = 5)
#This code is here so that Y (GPA) is capped at 4.0 (the logical max for GPA)
Y = (Y - min(Y)) / (max(Y) - min(Y))*4
#Finally, we put our data together with the data.frame() function
GPA.Data <- data.frame(GPA=Y, Work.Ethic=X, IQ=Z)
head(GPA.Data)
GPA.Data$IQ.C <- scale(GPA.Data$IQ, center = TRUE, scale = FALSE)[,]
GPA.Data$Work.Ethic.C <- scale(GPA.Data$Work.Ethic, center = TRUE, scale = FALSE)[,]
GPA.Model.1 <- lm(GPA~IQ.C+Work.Ethic.C, GPA.Data)
GPA.Model.2 <- lm (GPA~IQ.C*Work.Ethic.C, GPA.Data)
?effects
library(effects)
install.packages("effects")
library(effects)
#Run the interaction
Inter.HandPick <- effect('IQ.C*Work.Ethic.C', GPA.Model.2,
xlevels=list(IQ.C = c(-15, 0, 15),
Work.Ethic.C = c(-1.1, 0, 1.1)),
se=TRUE, confidence.level=.95, typical=mean)
#Put data in data frame
Inter.HandPick <- as.data.frame(Inter.HandPick)
#Check out what the "head" (first 6 rows) of your data looks like
head(Inter.HandPick)
#Create a factor of the IQ variable used in the interaction
Inter.HandPick$IQ <- factor(Inter.HandPick$IQ.C,
levels=c(-15, 0, 15),
labels=c("1 SD Below Population Mean", "Population Mean", "1 SD Above Population Mean"))
#Create a factor of the Work Ethic variable used in the interaction
Inter.HandPick$Work.Ethic <- factor(Inter.HandPick$Work.Ethic.C,
levels=c(-1.1, 0, 1.1),
labels=c("Poor Worker", "Average Worker", "Hard Worker"))
library(ggplot2)
Plot.HandPick<-ggplot(data=Inter.HandPick, aes(x=Work.Ethic, y=fit, group=IQ))+
geom_line(size=2, aes(color=IQ))+
ylim(0,4)+
ylab("GPA")+
xlab("Work Ethic")+
ggtitle("Hand Picked Plot")
Plot.HandPick
args(effect)
?"effect"
predict(GPA.Model.2, newdata = data.frame(IQ.C=c(-15, 0, 15), Work.Ethic.C=c(-1.1,0.1.1)), interval="confidence")
predict(GPA.Model.2, newdata = data.frame(IQ.C=-15, Work.Ethic.C=-1.1), interval="confidence")
head(Inter.HandPick)
Plot.HandPick
summary(clock.interact.lm)
lm(PRICE ~ AGE*NUMBIDS, data=GFCLOCKS)
summary(lm(PRICE ~ AGE*NUMBIDS, data=GFCLOCKS))
summary(clock.interact.lm)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 4 - Multiple Linear Regression")
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 4 - Multiple Linear Regression")
chickwts
chickData <- read.table("chickWeightGain.txt", header=TRUE)
head(chickData)
library(ggplot2)
ggplot(chickData, aes(x=diet, y=wtgain, group = diet, color=diet)) +
geom_boxplot()
setwd('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/ASSIGNMENTS/A1/')
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Rfunctions.R')
options(digits = 10, show.signif.stars = FALSE)
insectData <- read.table("insect.txt", header=TRUE)
head(insectData)
# part a) and b)
attach(insectData)
par(mfrow=c(1,1))
plot(Count ~ Ispray, main="Count of Surviving Insects After Each Insecticide")
# Outliers of insects after using sprays C, D
max(insectData$Count[which(insectData$Ispray == "C")])
max(insectData$Count[which(insectData$Ispray == "D")])
# part c) 1)
# (i)
insect.lm <- lm(Count ~ Ispray, data=insectData)
summary(insect.lm)
# (ii) Model assumptions
par(mfrow=c(1,2))
plot(insect.lm, which=1:2)
# Testing normality of errors with Shapiro wilk test - p-value is low
# must reject H0 of normality of residuals.
shapiro.test(insect.lm$residuals)
# part c) 2) Fitting the Square Root model
# (i)
insect.sqrt.lm <- lm(sqrt(Count) ~ Ispray, data=insectData)
summary(insect.sqrt.lm)
lm(sqrt(Count) ~ Ispray - 1, data=insectData)
summary(lm(sqrt(Count) ~ Ispray - 1, data=insectData))
summary(insect.sqrt.lm)
3.7606784+0.1159530
chick.lm <- lm(wtgain ~ diet, data=chickData)
summary(chick.lm)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 4 - Multiple Linear Regression")
starlingData <- read.table("starling.txt", header=TRUE)
head(starlingData)
View(starlingData)
starlingData$Roost <- as.factor(starlingData$Roost)
library(ggplot2)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", notchwidth = 1)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", notchwidth = 3)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", lwd = 3)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", lwd = 2)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", lwd = 1)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", size=3, lwd = 1)
geom_boxplot(outlier.colour = "red", lwd = 1)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", lwd = 1)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", outlier.size=0.5, lwd = 1)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", outlier.size=2, lwd = 1)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "red", outlier.size=4, lwd = 1)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "black", outlier.size=4, lwd = 1)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "black", outlier.size=4, lwd = 1) +
ggtitle("Effect of Roost on Starling Mass")
setwd('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/ASSIGNMENTS/A1/')
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Rfunctions.R')
options(digits = 5, show.signif.stars = FALSE)
hollyData <- read.table("Hollywood.txt", header=TRUE)
head(hollyData)
# part a) plotting with pairs()
pairs(hollyData[, c(2,3,4,1)], lower.panel = panel.smooth, upper.panel = panel.cor)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/")
## LECTURE PART ---------------------------------------------------------------
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Rfunctions.R')
# Plotting
aswellsData <- read.table("Chapter 4 - Multiple Linear Regression/aswells_short.txt", header=TRUE)
# the red line is just a smoother
# interpret: Arsenic-latitude - small decrease, corr = -0.183
# interpret 2: the predictor correlations between them are larger than
# predictor corrs with arsenic, the response.
# interpret 3: lots of scatter (variability) in the data - no strong relation
# seems to be present between predictor/predictors and predictors/arsenic.
pairs(aswellsData[, 1:4], lower.panel=panel.smooth, upper.panel = panel.cor)
# NOTE: if scatter is quadratic, corr is not useful since it only picks up
# on LINEAR association between two quantitative variables.
#
ggpairs(aswellsData, columns=1:4, upper=list(continuous="cor", params=c(size=10)),
lower=list(continuous="smooth", params=c(color="blue")))
library(GGally)
ggpairs(aswellsData, columns=1:4, upper=list(continuous="cor", params=c(size=10)),
lower=list(continuous="smooth", params=c(color="blue")))
ggplot(aswellsData, aes(x=LATITUDE, y=ARSENIC)) +
geom_point(shape=19, size=3, color="dodgerblue")
ggplot(aswellsData, aes(x=DEPTHFT, y=ARSENIC)) +
geom_point(shape=19, size=3, color="dodgerblue")
ggplot(aswellsData, aes(x=LATITUDE, y=LONGITUDE)) +
geom_point(shape=19, size=3, color="dodgerblue") +
geom_abline()
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ cyl + hp, data=df)
summary(lm_fit)
# save predictions of the model in the new data frame
# together with variable you want to plot against
predicted_df <- data.frame(mpg_pred = predict(lm_fit, df), hp=df$hp)
# this is the predicted line of multiple linear regression
ggplot(data = df, aes(x = mpg, y = hp)) +
geom_point(color='blue') +
geom_line(color='red',data = predicted_df, aes(x=mpg_pred, y=hp))
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ cyl + hp, data=df)
summary(lm_fit)
# save predictions of the model in the new data frame
# together with variable you want to plot against
predicted_df <- data.frame(mpg_pred = predict(lm_fit, df), hp=df$hp)
# this is the predicted line of multiple linear regression
ggplot(data = df, aes(x = mpg, y = hp)) +
geom_point(color='blue') +
geom_line(color='red',data = predicted_df, aes(x=mpg_pred, y=hp))
head(predict(lm_fit, df))
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ cyl + hp, data=df)
summary(lm_fit)
# save predictions of the model in the new data frame
# together with variable you want to plot against
predicted_df <- data.frame(mpg_pred = predict(lm_fit, df), hp=df$hp)
# this is the predicted line of multiple linear regression
ggplot(data = df, aes(x = mpg, y = hp)) +
geom_point(color='blue') +
geom_line(color='red',data = predicted_df, aes(x=hp, y=mpg_pred))
head(df)
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ cyl + hp, data=df)
summary(lm_fit)
# save predictions of the model in the new data frame
# together with variable you want to plot against
predicted_df <- data.frame(mpg_pred = predict(lm_fit, df), hp=df$hp)
# this is the predicted line of multiple linear regression
ggplot(data = df, aes(x = hp, y = mpg)) +
geom_point(color='blue') +
geom_line(color='red',data = predicted_df, aes(x=hp, y=mpg_pred))
lm_fit
mpg.lm <- lm(mpg ~ cyl + hp, data=df)
summary(mpg.lm)
data <- df;
data$hp <- seq(min(df$hp), max(df$hp), by=0.001)
data$hp <- linspace(min(df$hp), max(df$hp), n=nrow(df))
install.packages("pracma")
library(pracma)
data$hp <- linspace(min(df$hp), max(df$hp), n=nrow(df))
predicted_df <- data.frame(mpg_pred = predict(mpg.lm, data), hp=df$hp)
ggplot(data = df, aes(x = hp, y = mpg)) +
geom_point(color='blue') +
geom_line(color='red',data = predicted_df, aes(x=hp, y=mpg_pred))
ggplot(aswellsData, aes(x=LATITUDE, y=LONGITUDE)) +
geom_point(shape=19, size=3, color="dodgerblue") +
geom_abline()
load("data/Exercises and Examples/LIQUIDSPILL.Rdata")
reg.line <- lm(MASS ~ TIME, data=LIQUIDSPILL)
summary(reg.line)
View(LIQUIDSPILL)
ggplot(LIQUIDSPILL, aes(x=TIME, y=MASS)) + geom_point(shape=19, size=3,color="dodgerblue")
load("data/Exercises and Examples/FHWABRIDGE.Rdata")
plot(AREA ~ NUMBER, data=FHWABRIDGE)
ggplot(FHWABRIDGE, aes(x=NUMBER, y=AREA)) + geom_point(shape=19, size=3,color="dodgerblue")
receipts2.lm <- lm(Receipts ~ Production + Promo, data=hollyData)
library(ggfortify)
autoplot(receipts2.lm, which=1:2)
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "black", outlier.size=4, lwd = 1) +
ggtitle("Effect of Roost on Starling Mass")
starling.lm <- lm(Mass ~ Roost, data=starlingData)
summary(starling.lm)
83.60 -4.20
options(digits=10, show.signif.stars = FALSE)
starlingData <- read.table("starling.txt", header=TRUE)
head(starlingData)
starlingData$Roost <- as.factor(starlingData$Roost)
# Means don't seem significantly diffrent, just between Roost 4 and 1,
# and gradually lowering.
ggplot(starlingData, aes(x=Roost, y = Mass, group=Roost, color=Roost)) +
geom_boxplot(outlier.colour = "black", outlier.size=4, lwd = 1) +
ggtitle("Effect of Roost on Starling Mass")
# Linea rmodel
starling.lm <- lm(Mass ~ Roost, data=starlingData)
summary(starling.lm)
library(ggfortify)
autoplot(starling.lm, which=1:2)
View(starlingData)
min(starlingData$Mass)
max(starlingData$Mass)
starling.baseroost1.lm <- lm(Mass ~ Roost, data=starlingData)
starlingData.base3 <- starlingData
starlingData$Roost <- relevel(starlingData$Roost, ref="3")
starlingData <- read.table("starling.txt", header=TRUE)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 4 - Multiple Linear Regression")
starlingData <- read.table("starling.txt", header=TRUE)
starlingData.base3 <- starlingData
starlingData.base3$Roost <- relevel(starlingData$Roost, ref="3")
starlingData$Roost <- as.factor(starlingData$Roost)
starlingData.base3 <- starlingData
starlingData.base3$Roost <- relevel(starlingData$Roost, ref="3")
starling.baseroost3.lm <- lm(Mass ~ Roost, data=starlingData.base3)
summary(starlingData.base3)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 4 - Multiple Linear Regression/Worksheet_2_Starling_CategoricalPredictors.R', echo=TRUE)
starlingData$Roost <- relevel(starlingData$Roost, ref="3")
starlingData
attr(starlingData)
attributes(starlingData)
names(starlingData)
starling.baseroost3.lm <- lm(Mass ~ Roost, data=starlingData)
summary(starlingData.base3)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 4 - Multiple Linear Regression")
library(ggplot2)
options(digits=10, show.signif.stars = FALSE)
starlingData <- read.table("starling.txt", header=TRUE)
starlingData$Roost <- as.factor(starlingData$Roost)
head(starlingData)
starling1.lm <- lm(Mass ~ Roost, data=starlingData)
summary(starling1.lm)
# Base mean mass is Roost 1 (intercept) = 83.60 mass at roost 1
# mean mass at Roost 2 = roost2coef  + intercept = 83.6 - 4.20 = 79.4
# mean mass at roost 3 = roost3coef + intercept = 83.6 - 5 = 78.6
# mean mass at roost 4 = roost4coef + intercept = 83.60 - 8.20 = 75.40
autoplot(starling1.lm, which=1:2)
starlingData3 <- starlingData
starlingData3$Roost <- relevel(starlingData$Roost, ref="3")
starling3.lm <- lm(Mass ~ Roost, data=starlingData3)
summary(starling3.lm)
args(lm)
summary(starling1.lm)
summary(starling3.lm)
setwd('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/ASSIGNMENTS/A1/')
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Rfunctions.R')
options(digits = 10, show.signif.stars = FALSE)
insectData <- read.table("insect.txt", header=TRUE)
head(insectData)
# part a) and b)
attach(insectData)
par(mfrow=c(1,1))
plot(Count ~ Ispray, main="Count of Surviving Insects After Each Insecticide")
# Outliers of insects after using sprays C, D
max(insectData$Count[which(insectData$Ispray == "C")])
max(insectData$Count[which(insectData$Ispray == "D")])
View(insectData)
summary(starling1.lm)
autoplot(starling1.lm, which=1:2)
shapiro.test(starling1.lm$residuals)
summary(starling3.lm)
starling.nointercept.lm <- lm(Mass ~ Roost - 1, data=starlingData3)
betaCI(starling.nointercept.lm)
starling.nointercept.lm <- lm(Mass ~ Roost - 1, data=starlingData)
betaCI(starling.nointercept.lm)
betaCI(starling.nointercept.lm)
setwd('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/ASSIGNMENTS/A1/')
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Rfunctions.R')
options(digits = 5, show.signif.stars = FALSE)
hollyData <- read.table("Hollywood.txt", header=TRUE)
head(hollyData)
# part a) plotting with pairs()
pairs(hollyData[, c(2,3,4,1)], lower.panel = panel.smooth, upper.panel = panel.cor)
1-0.941
setwd('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/ASSIGNMENTS/A1/')
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Rfunctions.R')
options(digits = 10, show.signif.stars = FALSE)
insectData <- read.table("insect.txt", header=TRUE)
head(insectData)
# part a) and b)
attach(insectData)
par(mfrow=c(1,1))
plot(Count ~ Ispray, main="Count of Surviving Insects After Each Insecticide")
# Outliers of insects after using sprays C, D
max(insectData$Count[which(insectData$Ispray == "C")])
max(insectData$Count[which(insectData$Ispray == "D")])
# part c) 1)
# (i)
insect.lm <- lm(Count ~ Ispray, data=insectData)
summary(insect.lm)
# (ii) Model assumptions
par(mfrow=c(1,2))
plot(insect.lm, which=1:2)
shapiro.test(insect.lm$residuals)
options(digits = 5, show.signif.stars = FALSE)
insectData <- read.table("insect.txt", header=TRUE)
head(insectData)
# part a) and b)
attach(insectData)
par(mfrow=c(1,1))
plot(Count ~ Ispray, main="Count of Surviving Insects After Each Insecticide")
# Outliers of insects after using sprays C, D
max(insectData$Count[which(insectData$Ispray == "C")])
max(insectData$Count[which(insectData$Ispray == "D")])
# part c) 1)
# (i)
insect.lm <- lm(Count ~ Ispray, data=insectData)
summary(insect.lm)
# (ii) Model assumptions
par(mfrow=c(1,2))
plot(insect.lm, which=1:2)
# Testing normality of errors with Shapiro wilk test - p-value is low
# must reject H0 of normality of residuals.
shapiro.test(insect.lm$residuals)
insect.sqrt.lm <- lm(sqrt(Count) ~ Ispray, data=insectData)
summary(insect.sqrt.lm)
shapiro.test(insect.sqrt.lm$residuals)
par(mfrow=c(1,2))
plot(insect.sqrt.lm, which=1:2)
# Testing residuals normality - better now - we can say residuals are not
# deviating from normality.
shapiro.test(insect.sqrt.lm$residuals)
detach(insectData)
detach(insectData)
detach(insectData)
detach(insectData)
detach(insectData)
