m <- melt(df)
m <- melt(df, id=xs)
m <- melt(df, id.vars =xs)
m <- melt(df, id="xs")
head(m)
plotMultipleContinuousDist <- function(xs, ysList){
df <- data.frame(xs=xs)
df <- cbind(df, data.frame(sapply(ysList,c)))
library(reshape2)
df.melt <- melt(df, id="xs")
ggplot(data=df.melt, aes(x=xs, y=value, colour=variable)) + geom_line(size=2)
}
plotMultipleContinuousDist(xs, list(ys1, ys2, ys3))
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/PLOTTING_PROB.R', echo=TRUE)
plotMultipleContinuousDist(xs, list(ys1, ys2, ys3))
x <- runif(n=1000, min=0, max=1)
gx <- exp(x)
mean(gx)
f <- function(xx) exp(-xx)
integrate(f, lower=0, upper=1)
f <- function(xx) exp(xx)
integrate(f, lower=0, upper=1)
exactValue <- integrate(f, lower=0, upper=1)
exactValue$value
integrate(f, lower=0, upper=1)
integrate(function(xx) exp(xx), lower=0, upper=1)
estimateOfE = mean(gx) + 1
estimateOfE
x <- runif(n=10000, min=0, max=1)
gx <- exp(x)
mean(gx)
estimateOfE = mean(gx) + 1
estimateOfE
n <- 10000
x <- runif(n=n, min=0, max=1)
gx <- exp(x)
mean(gx)
estimateOfE = mean(gx) + 1
estimateOfE
(1/n)*sum(exp(x))
hx <- 1 / (1 + x^2)
mean(hx)
mean(hx)*4
simE <- 0 # store estimates of E
simPi <- 0 # store estimates of pi
# num samples
n <- 1000
for(i in 1:1000){
# gnerate sample of size n from Unif(0,1)
us <- runif(n, min=0, max=1)
EXPECT_eX <- (1/n)*sum(exp(us)) + 1
EeX <- (1/n)*sum(exp(us))
e <- 1 + Eex
simE[i] = e
# find E[1 / (1+x^2)]
EhX <- (1/n) * sum(1 / (1 + us^2))
# hx = arctan(1) = pi/4
pie <- EhX * 4
simPi[i] <- pie
}
for(i in 1:1000){
# gnerate sample of size n from Unif(0,1)
us <- runif(n, min=0, max=1)
EeX <- (1/n)*sum(exp(us))
e <- 1 + EeX
simE[i] = e
# find E[1 / (1+x^2)]
EhX <- (1/n) * sum(1 / (1 + us^2))
# hx = arctan(1) = pi/4
pie <- EhX * 4
simPi[i] <- pie
}
df <- data.frame(simE=simE, simPi=simPi)
df.melt <- melt(df)
df.melt
x <- data.frame(v1=rnorm(100),v2=rnorm(100,1,1),v3=rnorm(100,0,2))
m <- melt(x)
head(m)
m <- melt(x, id="")
df <- data.frame(simE=simE, simPi=simPi)
df.melt <- melt(df)
ggplot(df, aes(x=value, fill=variable)) + geom_density(alpha=0.2)
ggplot(df.melt, aes(x=value, fill=variable)) + geom_density(alpha=0.2)
ggplot(df.melt, aes(x=value, colour=variable)) + geom_density(size=1)
ggplot(df.melt, aes(x=value, colour=variable)) + geom_density(size=1)
mean(simPi)
mean(simE)
x <- seq(0, 10, by=0.01)
gamma1 <- dgamma(x=x, shape=2, scale=1)
gamma2 <- dgamma(x, shape=3, scale=1)
gamma3 <- dgamma(x, shape=4, scale1)
gamma3 <- dgamma(x, shape=4, scale=1)
gsList <- list(gamma1, gamma2, gamma3)
plotMultipleContinuousDist(x, gsList)
paste(alpha)
expression(paste(alpha))
leg.txt <- c(expression(paste("Gamma","(",alpha, "=","2",",",beta,"=",1,")")),
expression(paste("Gamma","(",alpha, "=","3",",",beta,"=",1,")")),
expression(paste("Gamma","(",alpha, "=","4",",",beta,"=",1,")")))
plot(x, gamma1, type="l", ylim=c(0,0.4), ylab="Density", main="Gamma Densities",las =1)
lins(x, gamma2, col=2)
lines(x, gamma2, col=2)
lines(x, gamma3, col=3)
legend(x=5.5, y=0.3, legend=leg.txt, fill=c(1,2,3))
randGamma <- rgamma(1000, shape=2, scale=3) # alpha=2, beta=3
mean(randGamma)
var(randGamma) # V(X) = a*b^2 = 2*9=18
qnorm(0.20, mean=0, sd=1)
qnorm(0.10, mean=0, sd=1)
pnorm(-1.3)
1.281552-0.8416212
-15/0.4399308
1.281552/0.8416212
20*1.281552/0.8416212-35
-4.545634/1.522718
qnorm(0.10, mean=0, sd=1, lower.tail=F)
qnorm(1 - 0.10, mean=0, sd=1) # another way
qnorm(0.20, mean=0, sd=1)
z0 <- qnorm(0.20, mean=0, sd=1)
z1 <- qnorm(0.10, mean=0, sd=1, lower.tail=F)
z0 <- qnorm(0.20, mean=0, sd=1); z0
z1 <- qnorm(0.10, mean=0, sd=1, lower.tail=F); z1
qexp(p=0.25, rate=1/40)
pexp(2, rate=9/4, lower.tail=F)
1 - pexp(2, rate=9/4)
ppois(2, lambda=9/4, lower.tail=F)
1 - ppois(2, lambda=9/4)
beta = 40
qexp(p=0.25, rate=1/beta)
pexp(90, rate=1/beta, lower.tail=F)
diff(pexp(c(10,20), rate=1/beta)) / pexp(10, rate=1/beta, lower.tail=F)
pexp(10, rate=1/beta)
pexp(10, rate=1/beta)
pbinom(0, size=5, p=pexp(10, rate=1/beta))
pbinom(0, size=5, p=pexp(10, rate=1/beta), lower.tail=FALSE)
p = pexp(10, rate=1/beta)
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p)
p = pexp(10, rate=1/beta, lower.tail=FALSE)
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p)
p = pexp(90, rate=1/beta, lower.tail=FALSE)
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p)
z0 <- qnorm(0.20, mean=0, sd=1); z0
z1 <- qnorm(0.10, mean=0, sd=1, lower.tail=F); z1
qnorm(1 - 0.10, mean=0, sd=1) # another way
pgamma(30000,shape=20,scale=1000,lower.tail=F)
integrate(function(x){(1/(1000^20)*gamma(20)) * x^(20-1) *exp(-x/1000) }, lower=30000, upper=Inf)
pgamma(30,000, shape=20, scale=1000, lower.tail=F) #
pgamma(30000, shape=20, scale=1000, lower.tail=F) #
gamma(2)
gamma(4)
gamma(5)
integrate(function(x){(1/(1000^20 * gamma(20))) * x^(20-1) *exp(-x/1000) }, lower=30000, upper=Inf)
gamma.density <- function(x){(1/(1000^20 * gamma(20))) * x^(20-1) *exp(-x/1000) }
integrate(gamma.density, lower=30000, upper=Inf)
pgamma(30000, shape=20, scale=1000, lower.tail=FALSE)
gamma(20)
factorial(19)
qexp(p=0.25, rate=1/beta)
pexp(90, rate=1/beta, lower.tail=F)
pexp(10, rate=1/beta)
diff(pexp(c(10,20), rate=1/beta)) / pexp(10, rate=1/beta, lower.tail=F)
p2 = pexp(10, rate=1/beta, lower.tail=F) # P (T > 10)
p1 / p2
p1 = diff(pexp(c(10,20), rate=1/beta)) # P(10 < T < 20)
p1 / p2
pexp(10, rate=1/beta)
ppois(2, lambda=9/4, lower.tail=F)
1 - ppois(2, lambda=9/4) # another way to calculate
p = pexp(90, rate=1/beta, lower.tail=FALSE)
pbinom(0, size=5, p=p, lower.tail=FALSE)
p = pexp(90, rate=1/beta, lower.tail=FALSE); p
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p)
diff(pbinom(c(145, 163), size=600, p=0.25))
diff(pbinom(c(144, 163), size=600, p=0.25))
diff(pnorm(c(144.5, 163.5), mean=600*0.25, sd=sqrt(600*0.25*0.75)))
n=10^5
X = runif(n=n, min=0, max=1)
Y = runif(n=n, min=0, max=1)
head(X)
probBothBetweenHalf <- sum((X <= 0.5) & (Y <= 0.5))/n
probBothBetweenHalf
n = 500 # sample of size 500 from bivariate uniform.
u1 <- runif(n=n, min=0, max=1)
u2 <- runif(n=n, min=0, max=1)
# logical vec depending on whether conditions are met
logicBothBetweenHalf <- (u1 < 05.) & (u2 < 0.5)
logicBothBetweenHalf
probBothBetweenHalf <- sum((u1 < 05.) & (u2 < 0.5))
probBothBetweenHalf
probBothBetweenHalf <- sum((u1 < 05.) & (u2 < 0.5)) / n
probBothBetweenHalf
theSim <- 0
library(reshape2)
theSim <- 0
# repeat sampling 1000 times
for(i in 1:1000) {
n = 500 # sample of size 500 from bivariate uniform.
u1 <- runif(n=n, min=0, max=1)
u2 <- runif(n=n, min=0, max=1)
# logical vec depending on whether conditions are met
probBothBetweenHalf <- sum((u1 < 05.) & (u2 < 0.5)) / n
#store result from each sample (i)
theSim[i] = probBothBetweenHalf
}
theSim
melt(data)
data <- melt(data.frame(theSim))
data
head(data)
ggplot(data, aes(y=value)) + geom_density()
library(ggpl2)
Library(ggplot2)
library(ggplot2)
ggplot(data, aes(y=value)) + geom_density()
data <- data.frame(theSim)
ggplot(data, aes(x=theSim)) + geom_density()
ggplot(data, aes(x=theSim)) + geom_density(size=2, colour="hotpink")
ggplot(data, aes(x=theSim)) + geom_histogram()
ggplot(data, aes(x=theSim)) + geom_histogram(colour="hotpink")
ggplot(data, aes(x=theSim)) + geom_histogram(fill="hotpink")
ggplot(data, aes(x=theSim)) + geom_histogram(fill="lightpink")
theSim <- 0
# repeat sampling 1000 times
for(i in 1:1000) {
n = 500 # sample of size 500 from bivariate uniform.
u1 <- runif(n=n, min=0, max=1)
u2 <- runif(n=n, min=0, max=1)
# logical vec depending on whether conditions are met
probBothBetweenHalf <- sum((u1 < 05.) & (u2 < 0.5)) / n
#store result from each sample (i)
theSim[i] = probBothBetweenHalf
}
library(ggplot2)
data <- data.frame(theSim)
ggplot(data, aes(x=theSim)) + geom_density(size=2, colour="hotpink")
ggplot(data, aes(x=theSim)) + geom_histogram(fill="lightpink")
sampleSize <- 10^4
die1 <- sample(x=1:6, size=sampleSize, replace=TRUE) # each prob = 1/6
die2 <- sample(x=1:6, size=sampleSize, replace=TRUE) # each prob = 1/6
jointProb <- table(die1, die2)/sampleSize
jointProb
table(die1, die2)
jointProb[5:6]
jointProb[5:6]
fives <- (die1 == 5) + (die2==5)
sixes <- (die1 == 6) + (die2 == 6)
jointProb56 <- table(fives, sixes)/sampleSize
jointProb56
diag(table(die1, die2)[5:6]/sampleSize)
jointProb[5:6] # this is the joint probability of
diag(table(die1, die2))[5:6]/sampleSize
jointProb56
prob56 <- diag(table(die1, die2))[5:6]/sampleSize
jointProb56
jointProb56
jointProb56[3,1]
jointProb56[1,3]
(2/3)^2
mu_I = 29.87
mu_C = 31.77
sd_I=7.71
sd_C = 7.86
p = 0.957
# part d) pdf of Canandaigua temperature (had muC, varC)
lower <- mu_C - 4 * sd_C
upper <- mu_C + 4 * sd_C
xs <- seq(lower, upper, length=10^3)
ys = dnorm(x=xs, mean=mu_C, sd=sd_C)
df <- data.frame(xs=xs, ys=ys)
ggplot() +
geom_line(data=df, aes(x=xs, y=ys), size=1, colour="blue") +
geom_vline(xintercept=mu_C, colour="black", linetype="dashed", size=1) +
ggtitle("Normal Density of Canandaigua Max Temperature")
mu_C_given_I <- function(i) mu_C + p * (sd_C/sd_I) * (i - mu_I)
sd_C_given_I <- sd_C^2 * (1 - p^2)
df.cond <- data.frame(xs=xs, ys=dnorm(x=xs, mean=mu_C_given_I(25), sd=sd_C_given_I))
ggplot() +
geom_line(data=df.cond, aes(x=xs, y=ys), size=1, colour="red") +
geom_vline(xintercept=mu_C_given_I(25), colour="black", linetype="dashed",
size=1) +
ggtitle("Conditional Normal PDF of Canadaigua given Ithaca = 25 degrees F")
x1 <- x2 <- seq(0, 1, length=128)
x1
x2
const <- 6/5
var.grid <- expand.grid("x1", =x1, "x2"=x2)# create grid of bivarate rvs
var.grid <- expand.grid("x1" =x1, "x2"=x2)# create grid of bivarate rvs
var.grid
X1 <- var.grid$x1
X2 <- var.grid$x2
X2
?expand.grid
x <- seq(0, 10, length.out = 100)
y <- seq(-1, 1, length.out = 20)
d1 <- expand.grid(x = x, y = y)
length(x)
length(y)
d1
head(d1)
head(d1, 103)
jointPDF <- const * (X1 + X2^2)
jointPDF
jointPDF <- matrix(jointPDF, ncol=128, nrow=128)
jointPDF
x11()
jointPDF # typecast as matrix.
x11()
persp(z=jointPDF, y=x1, x=x2, col="lightblue") # perspective plot
x11()
image(jointPDF, xlab="x1", ylab="x2", col=heat.colors(256))
# image view
axis(side=1, at = seq(0,1,by=0.2), labels=seq(0,1,length=6))
axis(side=2, at = seq(0,1,by=0.2), labels=seq(0,1,length=6))
contour(jointPDF, nlevels=10, add=TRUE)
indices <- which(X1 <= 0.5 & X2 <= 0.5)
indices
head(indices)
head(indices, 20)
regionOfInterest <- jointPDF[indices]
regionOfInterest
prob <- sum(regionOfInterest * (1/128^2)) # calculate probability using a riemann sum.
prob
N <- 128
z0 <- qnorm(0.20, mean=0, sd=1); z0
z1 <- qnorm(0.10, mean=0, sd=1, lower.tail=F); z1
qnorm(1 - 0.10, mean=0, sd=1) # another way
beta = 40
# part a) lower quartile: P(T <= t0) = 0.25
qexp(p=0.25, rate=1/beta)
pexp(90, rate=1/beta, lower.tail=F)
pexp(10, rate=1/beta)
p1 = diff(pexp(c(10,20), rate=1/beta)) # P(10 < T < 20)
p2 = pexp(10, rate=1/beta, lower.tail=F) # P (T > 10)
p1 / p2
pexp(10, rate=1/beta)
ppois(2, lambda=9/4, lower.tail=F)
1 - ppois(2, lambda=9/4) # another way to calculate
p = pexp(90, rate=1/beta, lower.tail=FALSE); p
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p) # another way to calculate.
diff(pbinom(c(144, 163), size=600, p=0.25))
diff(pnorm(c(144.5, 163.5), mean=600*0.25, sd=sqrt(600*0.25*0.75)))
gamma.density <- function(x){(1/(1000^20 * gamma(20))) * x^(20-1) *exp(-x/1000) }
integrate(gamma.density, lower=30000, upper=Inf)
res =integrate(gamma.density, lower=30000, upper=Inf)
res$value
result = integrate(gamma.density, lower=30000, upper=Inf)
result$value
pgamma(30000, shape=20, scale=1000, lower.tail=FALSE)
integrate(gamma.density, lower=30000, upper=Inf)
pgamma(30000, shape=20, scale=1000, lower.tail=FALSE)
N <- 10^4
dice1 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice2 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice3 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
diceSum <- dice1 + dice2 + dice3
probLessTen <- sum(diceSum < 10) / N; probLessTen
rolls <- cbind(dice1, dice2, dice3)
count.1 <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count.1 = count.1 + 1
}
}
# the probability we are finding is:
probFaceValuesAllDifferent.1 <- count.1 / N;
probFaceValuesAllDifferent.1
count.2 <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(length(unique(rolls[i, ])) == 3){
# then we increment the count
count.2 = count.2 + 1
}
}
probFaceValuesAllDifferent.2 <- count.2 / N
probFaceValuesAllDifferent.2
p2 = sum(dice1 != dice2 & dice1 != dice3 & dice2 != dice3)/N; p2
dice1 != dice2
0
1 - pbinom(100, size=105, prob = 0.90)
pbinom(100, size = 105, prob = 0.90, lower.tail=FALSE)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/ASSIGNMENTS/A4")
options(digits=10, show.signif.stars = FALSE)
oxygenData <- read.table("o2.txt", header=TRUE)
View(oxygenData)
oxygenData$temperature
n = nrow(oxygenData)
determine <- function(num){
if(num == 1) return("low")
else if(num == 2) return("medium")
else return("high")
}
sapply(oxygenData$temperature, determine)
cbind(oxygenData$temperature, sapply(oxygenData$temperature, determine))
numToTempLevel <- function(num){
if(num == 1) return("low")
else if(num == 2) return("medium")
else return("high")
}
oxygenData$temperature <- sapply(oxygenData$temperature, numToTempLevel)
oxygenData
oxygenData$species <- factor(oxygenData$species)
oxygenData
is.factor(oxygenData$temperature)
oxygenData$temperature <- factor(oxygenData$temperature)
is.factor(oxygenData$gender)
library(dae)
attach(oxygenData)
interaction.ABC.plot(response=resprate, x.factor=trace.factor=gender,
x.factor=temperature, groups.factor=species)
interaction.ABC.plot(response=resprate, trace.factor=gender,
x.factor=temperature, groups.factor=species)
detach(oxygenData)
interaction.ABC.plot(response=resprate, trace.factor=gender,
x.factor=temperature, groups.factor=species, data=oxygenData)
levels(oxygenData$temperature)
oxygenData$temperature <- relevel(oxygenData$temperature, ref="low")
levels(oxygenData$temperature)
melonData <- data.frame(Yield=c(25.12, 17.25, 26.42, 16.08, 22.15, 15.92,
40.25, 35.25, 31.98, 36.52, 43.32, 37.10,
18.3, 22.6, 25.9, 15.05, 11.42, 23.68,
28.55, 28.05, 33.2, 31.68, 30.32, 27.58),
Variety=c(rep("A",6),rep("B",6),rep("C",6),rep("D",6)))
melonData
weightData <- data.frame(Supplement=c(rep("1", 5), rep("2", 5), rep("3", 5)),
Litter=rep(1:5, 3),
WeightGain=c(28.7, 30.6, 27.2, 28.6, 31.9,
30.7, 345., 32.6, 34.4, 30.7,
25.4, 26.3, 27.5, 24.3, 25.8))
weightData
weightData <- data.frame(Supplement=c(rep("1", 5), rep("2", 5), rep("3", 5)),
DietaryLitter=rep(1:5, 3),
WeightGain=c(28.7, 30.6, 27.2, 28.6, 31.9,
30.7, 34.5, 32.6, 34.4, 30.7,
25.4, 26.3, 27.5, 24.3, 25.8))
weightData
library(effects)
with(weightData, table(Supplement, DietaryLitter))
margin.table(weightData, 1)
margin.table(weightData, c(1,2))
subset(weightData, Supplement=="1")
mean(subset(weightData, Supplement=="1")$WeightGain)
mean(subset(weightData, Supplement=="2")$WeightGain)
mean(subset(weightData, Supplement=="3")$WeightGain)
complete.weight.lm <- lm(WeightGain ~ Supplement + DietaryLitter, data=weightData)
summary(complete.weight.lm)
anova(complete.weight.lm)
complete.weight.lm <- lm(WeightGain ~ DietaryLitter + Supplement, data=weightData)
summary(complete.weight.lm)
anova(complete.weight.lm)
12v3 = C(weightData$Supplement, contr=c(1,1,-2))
supp12v3 = C(weightData$Supplement, contr=c(1,1,-2))
supp1v2 = C(weightData$Supplement, contr=c(1,-1,0))
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/FORMULAS.R', echo=TRUE)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/DATA.R', echo=TRUE)
supp.testDiffs.lm <- lm(WeightGain ~ Supplement + DietaryLitter, data=weightData)
supp.testDiffs.lm <- lm(WeightGain ~supp12v3 + supp1v2, data=weightData)
anova(supp.testDiffs.lm)
summary(supp.testDiffs.lm)
testContrastsOrthogonal(supp.testDiffs.lm)
supp12v3 = C(weightData$Supplement, contr=c(1,1,-2), 1)
supp1v2 = C(weightData$Supplement, contr=c(1,-1,0),  1)
supp.testDiffs.lm <- lm(WeightGain ~ supp12v3 + supp1v2, data=weightData)
anova(supp.testDiffs.lm)
summary(supp.testDiffs.lm)
anova(supp.testDiffs.lm) # yess, all significant.
testContrastsOrthogonal(supp.testDiffs.lm)
getConnection(supp.testDiffs.lm)
getContrastMatrix(supp.testDiffs.lm)
theContrasts <- getContrastMatrix(supp.testDiffs.lm); theContrasts
suppContrasts <- getContrastMatrix(supp.testDiffs.lm); suppContrasts
t(suppContrasts) %*% suppContrasts
library(ggplot2)
library(ggfortify)
autoplot(complete.weight.lm, which=1:6)
plot(complete.weight.lm, which=1:6)
par(mfrow=c(2,2))
par(mfrow=c(2,2))
plot(complete.weight.lm, which=1:4)
par(mfrow=c(1,2))
plot(complete.weight.lm, which=5:6)
plot(complete.weight.lm, which=5:6, cook.levels = c(0.1, 0.5, 1))
plot(complete.weight.lm, which=5:6, cook.levels = c(0.1, 0.2, 0.5))
plot(complete.weight.lm, which=1:4, add.smooth = FALSE)
par(mfrow=c(2,2))
plot(complete.weight.lm, which=1:4, add.smooth = FALSE)
par(mfrow=c(1,2))
plot(complete.weight.lm, which=5:6, cook.levels = c(0.1, 0.2, 0.5), add.smooth = FALSE)
par(mfrow=c(1,1))
plot(complete.weight.lm, which=1)
plot(complete.weight.lm, which=2)
plot(complete.weight.lm, which=3)
plot(complete.weight.lm, which=4)
plot(complete.weight.lm, which=5)
plot(complete.weight.lm, which=6)
plot(complete.weight.lm, which=4)
plot(complete.weight.lm, which=5)
plot(complete.weight.lm, which=6)
t(suppContrasts)
suppContrasts
complete.weight.lm
anova(complete.weight.lm)
anova(lm(WeightGain ~ Supplement + DietaryLitter, data=weightData))
anova(lm(WeightGain ~ DietaryLitter + Supplement, data = weightData))
