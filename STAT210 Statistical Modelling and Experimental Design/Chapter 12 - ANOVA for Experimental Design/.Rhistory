reductionRequired
value95Percentile
randNorm <- rnorm(n=N, mean=101, sd=5) # units in decibels
sorted <- sort(randNorm)
value95Percentile <- sorted[round(N * 0.95)]
value95Percentile
oldMean = 105
reductionRequired = abs(value95Percentile - oldMean)
reductionRequired
randNorm <- rnorm(n=N, mean=101, sd=5) # units in decibels
sorted <- sort(randNorm)
value95Percentile <- sorted[round(N * 0.95)]
oldMean = 105
reductionRequired = abs(value95Percentile - oldMean)
reductionRequired
estimatedNum95Percentile <- sorted[round(N * 0.95)]
estimatedNum95Percentile
number95Percentile = 105
reductionRequired = abs(estimatedNum95Percentile - number95Percentile)
reductionRequired
1-ppois(3,lambda=3)
pbinom(0, size=10,prob=0.135,lower.tail=F)
dbinom(0, size=10, prob=0.135)
pbinom(0, size=10, prob=0.135)
pnorm(0.022387, lower.tail=F)
pnorm((800.5-770)/sqrt(231), lower.tail=F)
pgeom(48,p=0.02,lower.tail=F)
pbinom(34, size=100, p = 0.3, lower.tail=F)
pnorm(34.5, mean=100*0.3, sd=sqrt(30*0.7), lower.tail=F)
pexp(10, rate=2.5, lower.tail=F)
pexp(10, rate=2/5, lower.tail=F)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/PLOTTING_PROB.R', echo=TRUE)
xs <- seq(from=-3, to=3, by=0.1)
ys1 <- dnorm(x=xs, mean=0, sd = 0.5)
ys2 <- dnorm(x=xs, mean=0, sd = 1)
ys3 <- dnorm(x=xs, mean=0, sd = 2)
plotContinuousDist(xs, ys1)
library(ggplot2)
plotContinuousDist(xs, ys1)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/PLOTTING_PROB.R', echo=TRUE)
plotContinuousDist(xs, ys1)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/PLOTTING_PROB.R', echo=TRUE)
data <- data.frame(xs=xs, ys=list(ys1,ys2,ys3))
head(data)
cbind(list(ys1,ys2,ys3))
vals = (list(ys1,ys2,ys3))
vals = (c(ys1,ys2,ys3))
vals
cbind(vals)
cbind(matrix(ys1,ys2,ys3,nrow=length(ys1), ncol=3))
vals = (list(ys1,ys2,ys3))
data.frame(cbind(vals))
sapply(vals, c)
t(sapply(vals, c))
data  = data.frame(sapply(vals, c))
head (data)
head(vals)
head (data)
head(ys1)
ysList = vals;
df <- data.frame(sapply(ysList, c))
df$xs <- xs
head(df)
df <- data.frame()
df <- data.frame(xs=xs)
df <- cbind(df, data.frame(sapply(ysList,c)))
head(df)
library(reshape2)
m <- melt(df)
m <- melt(df, id=xs)
m <- melt(df, id.vars =xs)
m <- melt(df, id="xs")
head(m)
plotMultipleContinuousDist <- function(xs, ysList){
df <- data.frame(xs=xs)
df <- cbind(df, data.frame(sapply(ysList,c)))
library(reshape2)
df.melt <- melt(df, id="xs")
ggplot(data=df.melt, aes(x=xs, y=value, colour=variable)) + geom_line(size=2)
}
plotMultipleContinuousDist(xs, list(ys1, ys2, ys3))
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/PLOTTING_PROB.R', echo=TRUE)
plotMultipleContinuousDist(xs, list(ys1, ys2, ys3))
x <- runif(n=1000, min=0, max=1)
gx <- exp(x)
mean(gx)
f <- function(xx) exp(-xx)
integrate(f, lower=0, upper=1)
f <- function(xx) exp(xx)
integrate(f, lower=0, upper=1)
exactValue <- integrate(f, lower=0, upper=1)
exactValue$value
integrate(f, lower=0, upper=1)
integrate(function(xx) exp(xx), lower=0, upper=1)
estimateOfE = mean(gx) + 1
estimateOfE
x <- runif(n=10000, min=0, max=1)
gx <- exp(x)
mean(gx)
estimateOfE = mean(gx) + 1
estimateOfE
n <- 10000
x <- runif(n=n, min=0, max=1)
gx <- exp(x)
mean(gx)
estimateOfE = mean(gx) + 1
estimateOfE
(1/n)*sum(exp(x))
hx <- 1 / (1 + x^2)
mean(hx)
mean(hx)*4
simE <- 0 # store estimates of E
simPi <- 0 # store estimates of pi
# num samples
n <- 1000
for(i in 1:1000){
# gnerate sample of size n from Unif(0,1)
us <- runif(n, min=0, max=1)
EXPECT_eX <- (1/n)*sum(exp(us)) + 1
EeX <- (1/n)*sum(exp(us))
e <- 1 + Eex
simE[i] = e
# find E[1 / (1+x^2)]
EhX <- (1/n) * sum(1 / (1 + us^2))
# hx = arctan(1) = pi/4
pie <- EhX * 4
simPi[i] <- pie
}
for(i in 1:1000){
# gnerate sample of size n from Unif(0,1)
us <- runif(n, min=0, max=1)
EeX <- (1/n)*sum(exp(us))
e <- 1 + EeX
simE[i] = e
# find E[1 / (1+x^2)]
EhX <- (1/n) * sum(1 / (1 + us^2))
# hx = arctan(1) = pi/4
pie <- EhX * 4
simPi[i] <- pie
}
df <- data.frame(simE=simE, simPi=simPi)
df.melt <- melt(df)
df.melt
x <- data.frame(v1=rnorm(100),v2=rnorm(100,1,1),v3=rnorm(100,0,2))
m <- melt(x)
head(m)
m <- melt(x, id="")
df <- data.frame(simE=simE, simPi=simPi)
df.melt <- melt(df)
ggplot(df, aes(x=value, fill=variable)) + geom_density(alpha=0.2)
ggplot(df.melt, aes(x=value, fill=variable)) + geom_density(alpha=0.2)
ggplot(df.melt, aes(x=value, colour=variable)) + geom_density(size=1)
ggplot(df.melt, aes(x=value, colour=variable)) + geom_density(size=1)
mean(simPi)
mean(simE)
x <- seq(0, 10, by=0.01)
gamma1 <- dgamma(x=x, shape=2, scale=1)
gamma2 <- dgamma(x, shape=3, scale=1)
gamma3 <- dgamma(x, shape=4, scale1)
gamma3 <- dgamma(x, shape=4, scale=1)
gsList <- list(gamma1, gamma2, gamma3)
plotMultipleContinuousDist(x, gsList)
paste(alpha)
expression(paste(alpha))
leg.txt <- c(expression(paste("Gamma","(",alpha, "=","2",",",beta,"=",1,")")),
expression(paste("Gamma","(",alpha, "=","3",",",beta,"=",1,")")),
expression(paste("Gamma","(",alpha, "=","4",",",beta,"=",1,")")))
plot(x, gamma1, type="l", ylim=c(0,0.4), ylab="Density", main="Gamma Densities",las =1)
lins(x, gamma2, col=2)
lines(x, gamma2, col=2)
lines(x, gamma3, col=3)
legend(x=5.5, y=0.3, legend=leg.txt, fill=c(1,2,3))
randGamma <- rgamma(1000, shape=2, scale=3) # alpha=2, beta=3
mean(randGamma)
var(randGamma) # V(X) = a*b^2 = 2*9=18
qnorm(0.20, mean=0, sd=1)
qnorm(0.10, mean=0, sd=1)
pnorm(-1.3)
1.281552-0.8416212
-15/0.4399308
1.281552/0.8416212
20*1.281552/0.8416212-35
-4.545634/1.522718
qnorm(0.10, mean=0, sd=1, lower.tail=F)
qnorm(1 - 0.10, mean=0, sd=1) # another way
qnorm(0.20, mean=0, sd=1)
z0 <- qnorm(0.20, mean=0, sd=1)
z1 <- qnorm(0.10, mean=0, sd=1, lower.tail=F)
z0 <- qnorm(0.20, mean=0, sd=1); z0
z1 <- qnorm(0.10, mean=0, sd=1, lower.tail=F); z1
qexp(p=0.25, rate=1/40)
pexp(2, rate=9/4, lower.tail=F)
1 - pexp(2, rate=9/4)
ppois(2, lambda=9/4, lower.tail=F)
1 - ppois(2, lambda=9/4)
beta = 40
qexp(p=0.25, rate=1/beta)
pexp(90, rate=1/beta, lower.tail=F)
diff(pexp(c(10,20), rate=1/beta)) / pexp(10, rate=1/beta, lower.tail=F)
pexp(10, rate=1/beta)
pexp(10, rate=1/beta)
pbinom(0, size=5, p=pexp(10, rate=1/beta))
pbinom(0, size=5, p=pexp(10, rate=1/beta), lower.tail=FALSE)
p = pexp(10, rate=1/beta)
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p)
p = pexp(10, rate=1/beta, lower.tail=FALSE)
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p)
p = pexp(90, rate=1/beta, lower.tail=FALSE)
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p)
z0 <- qnorm(0.20, mean=0, sd=1); z0
z1 <- qnorm(0.10, mean=0, sd=1, lower.tail=F); z1
qnorm(1 - 0.10, mean=0, sd=1) # another way
pgamma(30000,shape=20,scale=1000,lower.tail=F)
integrate(function(x){(1/(1000^20)*gamma(20)) * x^(20-1) *exp(-x/1000) }, lower=30000, upper=Inf)
pgamma(30,000, shape=20, scale=1000, lower.tail=F) #
pgamma(30000, shape=20, scale=1000, lower.tail=F) #
gamma(2)
gamma(4)
gamma(5)
integrate(function(x){(1/(1000^20 * gamma(20))) * x^(20-1) *exp(-x/1000) }, lower=30000, upper=Inf)
gamma.density <- function(x){(1/(1000^20 * gamma(20))) * x^(20-1) *exp(-x/1000) }
integrate(gamma.density, lower=30000, upper=Inf)
pgamma(30000, shape=20, scale=1000, lower.tail=FALSE)
gamma(20)
factorial(19)
qexp(p=0.25, rate=1/beta)
pexp(90, rate=1/beta, lower.tail=F)
pexp(10, rate=1/beta)
diff(pexp(c(10,20), rate=1/beta)) / pexp(10, rate=1/beta, lower.tail=F)
p2 = pexp(10, rate=1/beta, lower.tail=F) # P (T > 10)
p1 / p2
p1 = diff(pexp(c(10,20), rate=1/beta)) # P(10 < T < 20)
p1 / p2
pexp(10, rate=1/beta)
ppois(2, lambda=9/4, lower.tail=F)
1 - ppois(2, lambda=9/4) # another way to calculate
p = pexp(90, rate=1/beta, lower.tail=FALSE)
pbinom(0, size=5, p=p, lower.tail=FALSE)
p = pexp(90, rate=1/beta, lower.tail=FALSE); p
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p)
diff(pbinom(c(145, 163), size=600, p=0.25))
diff(pbinom(c(144, 163), size=600, p=0.25))
diff(pnorm(c(144.5, 163.5), mean=600*0.25, sd=sqrt(600*0.25*0.75)))
n=10^5
X = runif(n=n, min=0, max=1)
Y = runif(n=n, min=0, max=1)
head(X)
probBothBetweenHalf <- sum((X <= 0.5) & (Y <= 0.5))/n
probBothBetweenHalf
n = 500 # sample of size 500 from bivariate uniform.
u1 <- runif(n=n, min=0, max=1)
u2 <- runif(n=n, min=0, max=1)
# logical vec depending on whether conditions are met
logicBothBetweenHalf <- (u1 < 05.) & (u2 < 0.5)
logicBothBetweenHalf
probBothBetweenHalf <- sum((u1 < 05.) & (u2 < 0.5))
probBothBetweenHalf
probBothBetweenHalf <- sum((u1 < 05.) & (u2 < 0.5)) / n
probBothBetweenHalf
theSim <- 0
library(reshape2)
theSim <- 0
# repeat sampling 1000 times
for(i in 1:1000) {
n = 500 # sample of size 500 from bivariate uniform.
u1 <- runif(n=n, min=0, max=1)
u2 <- runif(n=n, min=0, max=1)
# logical vec depending on whether conditions are met
probBothBetweenHalf <- sum((u1 < 05.) & (u2 < 0.5)) / n
#store result from each sample (i)
theSim[i] = probBothBetweenHalf
}
theSim
melt(data)
data <- melt(data.frame(theSim))
data
head(data)
ggplot(data, aes(y=value)) + geom_density()
library(ggpl2)
Library(ggplot2)
library(ggplot2)
ggplot(data, aes(y=value)) + geom_density()
data <- data.frame(theSim)
ggplot(data, aes(x=theSim)) + geom_density()
ggplot(data, aes(x=theSim)) + geom_density(size=2, colour="hotpink")
ggplot(data, aes(x=theSim)) + geom_histogram()
ggplot(data, aes(x=theSim)) + geom_histogram(colour="hotpink")
ggplot(data, aes(x=theSim)) + geom_histogram(fill="hotpink")
ggplot(data, aes(x=theSim)) + geom_histogram(fill="lightpink")
theSim <- 0
# repeat sampling 1000 times
for(i in 1:1000) {
n = 500 # sample of size 500 from bivariate uniform.
u1 <- runif(n=n, min=0, max=1)
u2 <- runif(n=n, min=0, max=1)
# logical vec depending on whether conditions are met
probBothBetweenHalf <- sum((u1 < 05.) & (u2 < 0.5)) / n
#store result from each sample (i)
theSim[i] = probBothBetweenHalf
}
library(ggplot2)
data <- data.frame(theSim)
ggplot(data, aes(x=theSim)) + geom_density(size=2, colour="hotpink")
ggplot(data, aes(x=theSim)) + geom_histogram(fill="lightpink")
sampleSize <- 10^4
die1 <- sample(x=1:6, size=sampleSize, replace=TRUE) # each prob = 1/6
die2 <- sample(x=1:6, size=sampleSize, replace=TRUE) # each prob = 1/6
jointProb <- table(die1, die2)/sampleSize
jointProb
table(die1, die2)
jointProb[5:6]
jointProb[5:6]
fives <- (die1 == 5) + (die2==5)
sixes <- (die1 == 6) + (die2 == 6)
jointProb56 <- table(fives, sixes)/sampleSize
jointProb56
diag(table(die1, die2)[5:6]/sampleSize)
jointProb[5:6] # this is the joint probability of
diag(table(die1, die2))[5:6]/sampleSize
jointProb56
prob56 <- diag(table(die1, die2))[5:6]/sampleSize
jointProb56
jointProb56
jointProb56[3,1]
jointProb56[1,3]
(2/3)^2
mu_I = 29.87
mu_C = 31.77
sd_I=7.71
sd_C = 7.86
p = 0.957
# part d) pdf of Canandaigua temperature (had muC, varC)
lower <- mu_C - 4 * sd_C
upper <- mu_C + 4 * sd_C
xs <- seq(lower, upper, length=10^3)
ys = dnorm(x=xs, mean=mu_C, sd=sd_C)
df <- data.frame(xs=xs, ys=ys)
ggplot() +
geom_line(data=df, aes(x=xs, y=ys), size=1, colour="blue") +
geom_vline(xintercept=mu_C, colour="black", linetype="dashed", size=1) +
ggtitle("Normal Density of Canandaigua Max Temperature")
mu_C_given_I <- function(i) mu_C + p * (sd_C/sd_I) * (i - mu_I)
sd_C_given_I <- sd_C^2 * (1 - p^2)
df.cond <- data.frame(xs=xs, ys=dnorm(x=xs, mean=mu_C_given_I(25), sd=sd_C_given_I))
ggplot() +
geom_line(data=df.cond, aes(x=xs, y=ys), size=1, colour="red") +
geom_vline(xintercept=mu_C_given_I(25), colour="black", linetype="dashed",
size=1) +
ggtitle("Conditional Normal PDF of Canadaigua given Ithaca = 25 degrees F")
x1 <- x2 <- seq(0, 1, length=128)
x1
x2
const <- 6/5
var.grid <- expand.grid("x1", =x1, "x2"=x2)# create grid of bivarate rvs
var.grid <- expand.grid("x1" =x1, "x2"=x2)# create grid of bivarate rvs
var.grid
X1 <- var.grid$x1
X2 <- var.grid$x2
X2
?expand.grid
x <- seq(0, 10, length.out = 100)
y <- seq(-1, 1, length.out = 20)
d1 <- expand.grid(x = x, y = y)
length(x)
length(y)
d1
head(d1)
head(d1, 103)
jointPDF <- const * (X1 + X2^2)
jointPDF
jointPDF <- matrix(jointPDF, ncol=128, nrow=128)
jointPDF
x11()
jointPDF # typecast as matrix.
x11()
persp(z=jointPDF, y=x1, x=x2, col="lightblue") # perspective plot
x11()
image(jointPDF, xlab="x1", ylab="x2", col=heat.colors(256))
# image view
axis(side=1, at = seq(0,1,by=0.2), labels=seq(0,1,length=6))
axis(side=2, at = seq(0,1,by=0.2), labels=seq(0,1,length=6))
contour(jointPDF, nlevels=10, add=TRUE)
indices <- which(X1 <= 0.5 & X2 <= 0.5)
indices
head(indices)
head(indices, 20)
regionOfInterest <- jointPDF[indices]
regionOfInterest
prob <- sum(regionOfInterest * (1/128^2)) # calculate probability using a riemann sum.
prob
N <- 128
z0 <- qnorm(0.20, mean=0, sd=1); z0
z1 <- qnorm(0.10, mean=0, sd=1, lower.tail=F); z1
qnorm(1 - 0.10, mean=0, sd=1) # another way
beta = 40
# part a) lower quartile: P(T <= t0) = 0.25
qexp(p=0.25, rate=1/beta)
pexp(90, rate=1/beta, lower.tail=F)
pexp(10, rate=1/beta)
p1 = diff(pexp(c(10,20), rate=1/beta)) # P(10 < T < 20)
p2 = pexp(10, rate=1/beta, lower.tail=F) # P (T > 10)
p1 / p2
pexp(10, rate=1/beta)
ppois(2, lambda=9/4, lower.tail=F)
1 - ppois(2, lambda=9/4) # another way to calculate
p = pexp(90, rate=1/beta, lower.tail=FALSE); p
pbinom(0, size=5, p=p, lower.tail=FALSE)
1 - dbinom(0, size=5, p=p) # another way to calculate.
diff(pbinom(c(144, 163), size=600, p=0.25))
diff(pnorm(c(144.5, 163.5), mean=600*0.25, sd=sqrt(600*0.25*0.75)))
gamma.density <- function(x){(1/(1000^20 * gamma(20))) * x^(20-1) *exp(-x/1000) }
integrate(gamma.density, lower=30000, upper=Inf)
res =integrate(gamma.density, lower=30000, upper=Inf)
res$value
result = integrate(gamma.density, lower=30000, upper=Inf)
result$value
pgamma(30000, shape=20, scale=1000, lower.tail=FALSE)
integrate(gamma.density, lower=30000, upper=Inf)
pgamma(30000, shape=20, scale=1000, lower.tail=FALSE)
N <- 10^4
dice1 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice2 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice3 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
diceSum <- dice1 + dice2 + dice3
probLessTen <- sum(diceSum < 10) / N; probLessTen
rolls <- cbind(dice1, dice2, dice3)
count.1 <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count.1 = count.1 + 1
}
}
# the probability we are finding is:
probFaceValuesAllDifferent.1 <- count.1 / N;
probFaceValuesAllDifferent.1
count.2 <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(length(unique(rolls[i, ])) == 3){
# then we increment the count
count.2 = count.2 + 1
}
}
probFaceValuesAllDifferent.2 <- count.2 / N
probFaceValuesAllDifferent.2
p2 = sum(dice1 != dice2 & dice1 != dice3 & dice2 != dice3)/N; p2
dice1 != dice2
0
1 - pbinom(100, size=105, prob = 0.90)
pbinom(100, size = 105, prob = 0.90, lower.tail=FALSE)
tulipData <- read.table("tulips.txt", header=TRUE)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 12 - ANOVA for Experimental Design")
tulipData <- read.table("tulips.txt", header=TRUE)
tulipData
tulipData$BED <- factor(tulipData$BED)
tulipData$SHADE <- factor(tulipData$SHADE)
tulipData$WATER <- factor(tulipData$WATER)
interaction.plot(x.factor=WATER, trace.factor=SHADE, response=BLOOMS)
interaction.plot(x.factor=WATER, trace.factor=SHADE, response=BLOOMS, data=tulipData)
with(tulipData, interaction.plot(x.factor=WATER, trace.factor=SHADE, response=BLOOMS)
with(tulipData, interaction.plot(x.factor=WATER, trace.factor=SHADE, response=BLOOMS))
with(tulipData, interaction.plot(x.factor=WATER, trace.factor=SHADE, response=BLOOMS))
tulip.lm <- lm(BLOOMS ~ BED + WATER*SHADE, data=tulipData)
library(effects)
eff.tulip <- allEffects(tulip.lm)
plot(eff.tulip)
eff.tulip
with(tulipData, tapply(BLOOMS, INDEX=list(WATER, SHADE), mean))
anova(eff.tulip)
anova(tulip.lm)
eff.tulip
options(show.signif.stars = FALSE)
anova(tulip.lm)
summary(tulip.lm)
melonData <- data.frame(Yield=c(25.12, 17.25, 26.42, 16.08, 22.15, 15.92,
40.25, 35.25, 31.98, 36.52, 43.32, 37.10,
18.3, 22.6, 25.9, 15.05, 11.42, 23.68,
28.55, 28.05, 33.2, 31.68, 30.32, 27.58),
Variety=c(rep("A",6),rep("B",6),rep("C",6),rep("D",6)))
melonData
melon.lm <- lm(Yield ~ Variety, data=melonData, x=TRUE)
summary(melon.lm)
anova(melon.lm)
betaCI(melon.lm)
melon.lm$x
contrasts(melonData$Variety)
# new contrasts
ACvBD <- C(melonData$Variety, contr=c(1,-1,1,-1), how.many=1)
AvC <- C(melonData$Variety, contr=c(1, 0, -1, 0), 1)
BvD <- C(melonData$Variety, c(0,1,0,-1), 1)
# check they are orthogonal:
mat <- matrix(c(1,-1,1,-1, 1,0,-1,0,  0,1,0,-1), ncol=3)
rownames(mat) <- c("A", "B", "C", "D")
colnames(mat) <- c("ACvBD", "AvC", "BvD")
#  can check with transpose
t(mat) %*% mat
# refit the model with the orthogonal contrasts to partition variety SS
melon.orthog.lm <- lm(Yield ~ ACvBD + AvC + BvD, data=melonData, x=TRUE)
# comparing the contrasts:
contrasts(melonData$Variety) # for original model
melon.orthog.lm$contrasts # for the orthogonal model
betaCI(melon.orthog.lm)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/FORMULAS.R', echo=TRUE)
betaCI(melon.orthog.lm)
ACvBD
