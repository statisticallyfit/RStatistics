# There are 3 intervals here where at least 5 customers entered
sim.results <- table(customers); sim.results
sum(sim.results >= 5)
data.frame(PSim=p.sim, P=p)
sum(c(13, 19, 32, 17, 10,  5,  2,  2))
N <- 1000
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = N, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / N; p.sim
p
# There are 3 intervals here where at least 5 customers entered
sim.results <- table(customers); sim.results
data.frame(PSim=p.sim, P=p)
# Unlikely P(X >= 5) can suspect that mu = 2 may be greater than 2
# if indeed we think five or more customers arrive in a half minute interval
# in our observation.
N <- 1000
# Poisson lambda = 2
lambda = 2 # 2 customers per half minute
# Probability 5 or more customers in half minute interval
# These are the same:
p = ppois(5-1, lambda=lambda, lower.tail=FALSE); p
1 - ppois(4, lambda=lambda)
# Simulating arrival of 100 customers in the half minute intervals, estimate P(X >= 5)
customers <- rpois(n = N, lambda=lambda)
customers[10] # number of customers arriving in the tenth half minute interval
mean(customers) # close to 2
hist(customers)
p.sim <- sum(customers >= 5) / N; p.sim
p
# There are 3 intervals here where at least 5 customers entered
sim.results <- table(customers); sim.results
data.frame(PSim=p.sim, P=p)
# Unlikely P(X >= 5) can suspect that mu = 2 may be greater than 2
# if indeed we think five or more customers arrive in a half minute interval
# in our observation.
w <- 1:3
rbind(w, pmf.W(w))
# Question 1
pmf.W <- function(w) {
5*w / (6 * (1 + w^2))
}
# a) is valid PDF since it sums to 1
sum(pmf.W(1) + pmf.W(2) + pmf.W(3))
# and all values are positive
all(c(pmf.W(1) > 0, pmf.W(2) > 0, pmf.W(3) > 0))
# another way to check all values are positive
w <- 1:3
rbind(w, pmf.W(w))
# b) find E(W)
mu.W <- pmf.W(1) * 1 + pmf.W(2) * 2 + pmf.W(3) * 3
mu.W
# c) var(W)
# E(W^2)
mu2.W <- pmf.W(1) * 1^2 + pmf.W(2) * 2^2 + pmf.W(3) * 3^3
mu2.W
# V(W)
var.w <- mu2.W - mu.W^2
var.w
# d) TODO why not the same as above?
var.another.W <- pmf.W(1)*(1-mu.W)^2 + pmf.W(2)*(2-mu.W)^2 + pmf.W(3)*(3-mu.W)^2
var.another.W
# NUMBER 2 Integration
f.X <- function(x) { 2*(1 - x) }
# a)
integrate(f.X, 0, 1) # so is valid prob func
# b)
f.ex <- function(x) {2*x*(1-x)}
EX <- integrate(f.ex, 0, 1)
EX
names(EX)
EX$value
# c)
f.var <- function(x) { (x - EX$value)^2 * 2 * (1 - x)}
VAR.X <- integrate(f.var, 0, 1)
VAR.X$value
# NUMBER 3 Dice
# a)
diceSample <- sample(1:6, size=120, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
table(diceSample)/120
# b)
mean(diceSample) # true = 3.5
sqrt(var(diceSample)) # true = 1.708
# c)
barplot(table(diceSample))
hist(diceSample)
library(ggplot2)
df <- data.frame(diceSample)
ggplot(df, aes(diceSample)) + geom_histogram(binwidth=1, fill='dodgerblue')
rbind(w, pmf.W(w))
pmf.W(w)
sum(pmf.W(w))
mu.W <- pmf.W(1) * 1 + pmf.W(2) * 2 + pmf.W(3) * 3
mu.W
w*pmf.W(w)
sum(w*pmf.W(w))
sum(w^2 * pmf.W(w)) - mu.W^2
var.w <- mu2.W - mu.W^2
var.w
var.W <- sum(w^2 * pmf.W(w)) - mu.W^2
var.W
var.W <- sum(w^2 * pmf.W(w)) - mu.W^2
var.W
var.another.W <- pmf.W(1)*(1-mu.W)^2 + pmf.W(2)*(2-mu.W)^2 + pmf.W(3)*(3-mu.W)^2
var.another.W
f.X <- function(x) { 2*(1 - x) }
# a)
integrate(f.X, 0, 1) # so is valid prob func
f.ex <- function(x) {2*x*(1-x)}
EX <- integrate(f.ex, 0, 1)
EX
names(EX)
EX$value
# c)
f.var <- function(x) { (x - EX$value)^2 * 2 * (1 - x)}
VAR.X <- integrate(f.var, 0, 1)
VAR.X$value
diceSample <- sample(1:6, size=120, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
table(diceSample)/120
# b)
mean(diceSample) # true = 3.5
sqrt(var(diceSample)) # true = 1.708
sd(diceSample)
barplot(table(diceSample))
dice1 <- sample(1:6, size=10^4, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice2 <- sample(1:6, size=10^4, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice3 <- sample(1:6, size=10^4, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice1
diceSum <- dice1 + dice2 + dice3
diceSum
N <- 10^4
probLessTen <- sum(diceSum < 10) / N; probLessTen
head(dice1, dice2, dice3)
head(cbind(dice1, dice2, dice3))
df <- cbind(dice1, dice2, dice3)
df[6,]
unique(df[6,])
df[6,1] == df[6,2] == df[6,3]
df[6,1] == df[6,2] || df[6,1] == df[6,3] || df[6,2] == df[6,3]
rolls <- cbind(dice1, dice2, dice3)
c = -
c = 0
c = 0
c += 1
probFaceValuesAllDifferent
probFaceValuesAllDifferent <- count / N;
probFaceValuesAllDifferent
probFaceValuesAllDifferent <- count / N;
rolls <- cbind(dice1, dice2, dice3)
#unique(df[6,])
count <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count = count + 1
}
}
# the probability we are finding is:
probFaceValuesAllDifferent <- count / N;
probFaceValuesAllDifferent
count <- 0
unique(rolls)
head(unique(rolls))
head(rolls)
count <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(unique(rolls[i, ]) == 3){
# then we increment the count
count = count + 1
}
}
count /
count / N
count <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(unique(rolls[i, ]) == 3){
# then we increment the count
count = count + 1
}
}
count / N
count <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count = count + 1
}
}
count
count <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(unique(rolls[i, ]) == 3){
# then we increment the count
count = count + 1
}
}
warnings()
count <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(length(unique(rolls[i, ])) == 3){
# then we increment the count
count = count + 1
}
}
count
count.1 <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count.1 = count.1 + 1
}
}
# the probability we are finding is:
probFaceValuesAllDifferent.1 <- count.1 / N;
probFaceValuesAllDifferen.1
probFaceValuesAllDifferent.1
count.2 <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(length(unique(rolls[i, ])) == 3){
# then we increment the count
count.2 = count.2 + 1
}
}
probFaceValuesAllDifferent.2 <- count.2 / N
assertthat::count.1 == count.2
assertthat(count.1 == count.2)
assert_that(count.1 == count.2)
count.1 == count.2
probFaceValuesAllDifferent.2
1 - pbinom(100, size=105, n = 0.10)
1 - pbinom(100, size=105, prob = 0.10)
pbinom(100, size = 105, prob = 0.10, lower.tail=FALSE)
1 - pbinom(100, size=105, prob = 0.90)
pbinom(100, size = 105, prob = 0.90, lower.tail=FALSE)
ppois(11, lambda=10, lower.tail = FALSE)
1 - ppois(11, lambda=10)
p = ppois(11, lambda=10, lower.tail = FALSE)
pbinom(2, size=8, prob=p, lower.tail = FALSE)
1 - pbinom(2, size=8, prob=p)
p = pgeom(2, prob=0.8, lower.tail = FALSE)
p = pgeom(2, prob=0.8, lower.tail = FALSE); p
p = pgeom(q=2, prob=0.8, lower.tail = FALSE); p
1 - pgeom(2, prob=0.8)
p = pgeom(q=3, prob=0.8, lower.tail = FALSE); p
p = pgeom(q=1, prob=0.8, lower.tail = FALSE); p
?pgeom
p = pgeom(q=2, prob=0.8, lower.tail = FALSE); p
p = pgeom(q=1, prob=0.8, lower.tail = FALSE); p
1 - pgeom(1, prob=0.8)
pbinom(3, size=10, prob = p, lower.tail=FALSE)
1 - pbinom(3, size=10, prob=p)
N <- 10^4
dice1 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice2 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
dice3 <- sample(1:6, size=N, replace=TRUE, prob=c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))
diceSum <- dice1 + dice2 + dice3
# part a) Probability that sum of face values < 10
probLessTen <- sum(diceSum < 10) / N; probLessTen
rolls <- cbind(dice1, dice2, dice3)
count.1 <- 0
for (i in 1:N){
# if all of the rolls of the 3 dice are not the same for this iteration i,
if(rolls[i, 1] != rolls[i,2] && rolls[i,1] != rolls[i, 3] &&
rolls[i,2] != rolls[i,3]){
# then we do increment the count
count.1 = count.1 + 1
}
}
# the probability we are finding is:
probFaceValuesAllDifferent.1 <- count.1 / N;
probFaceValuesAllDifferent.1
count.2 <- 0
for (i in 1:N){
# if all of the rolls for this row i are unique,
if(length(unique(rolls[i, ])) == 3){
# then we increment the count
count.2 = count.2 + 1
}
}
probFaceValuesAllDifferent.2 <- count.2 / N
probFaceValuesAllDifferent.2
# Test to make sure: Should be true, the two methods yield the same answer.
count.1 == count.2
1 - pbinom(100, size=105, prob = 0.90)
# method 2 of calculating
pbinom(100, size = 105, prob = 0.90, lower.tail=FALSE)
p = ppois(11, lambda=10, lower.tail = FALSE)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/MATH260 Probability and Simulation/Assignment2_questions_1_4_5.R', echo=TRUE)
p = ppois(11, lambda=10, lower.tail = FALSE); p
1 - ppois(11, lambda=10)
pbinom(100, size = 105, prob = 0.90, lower.tail=FALSE)
p = ppois(11, lambda=10, lower.tail = FALSE); p
# method 2
1 - ppois(11, lambda=10)
pbinom(2, size=8, prob=p, lower.tail = FALSE)
# method 2
1 - pbinom(2, size=8, prob=p)
p = pgeom(q=1, prob=0.8, lower.tail = FALSE); p
# Method 2
1 - pgeom(1, prob=0.8)
pbinom(3, size=10, prob = p, lower.tail=FALSE)
# Method 2
1 - pbinom(3, size=10, prob=p)
p = pgeom(q=0, prob=0.8, lower.tail = FALSE); p
1 - pgeom(1, prob=0.8) # P(X >= 2) = P(X <= 1)
1 - pgeom(2, prob=0.8) # P(X >= 2) = P(X <= 1)
1 - pgeom(0, prob=0.8) # P(X >= 2) = P(X <= 1)
pbinom(3, size=10, prob = p, lower.tail=FALSE)
1 - pbinom(3, size=10, prob=p)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/PRACTICALS/Practical_3_Rats/Practical_3_Rats_PolyReg.R', echo=TRUE)
library(ggplot2)
options(digits = 10, show.signif.stars = FALSE)
conc <- c(rep(seq(from = 0.5, to = 3, by = 0.5), 2))
conc #
# ys = skin response in rats
skin <- c(13.9, 14.08, 13.75, 13.32, 13.45, 13.59, 13.81, 13.99, 13.60, 13.39,
13.53, 13.64)
ratsData <- data.frame(Skin=skin, Conc=conc)
# Plotting data
plot(skin ~ conc) # seems negative x^3 model with B3 < 0
ggplot(ratsData, aes(x = Conc, y = Skin)) +
geom_point(shape=19, size=3, color="dodgerblue")
skin2.lm <- lm(Skin ~ Conc + I(Conc^2), data=ratsData)
summary(skin2.lm)
anova(skin2.lm)
lin <- lm(Skin ~ Conc, data=ratsData)
quad <- lm(Skin ~ Conc + I(Conc^2), data=ratsData)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/FORMULAS.R', echo=TRUE)
NestedFandChiSqTest(lin, quad)
summary(skin2.lm)
anova(skin2.lm)
NestedFandChiSqTest(lin, quad)
skin3.lm <- lm(Skin ~ Conc + I(Conc^2) + I(Conc^3), data=ratsData)
summary(skin3.lm)
anova(skin3.lm) # seeing that after fitting the linear and quadratic term,
cub <- update(lin, . ~. + I(Conc^3), data=ratsData)
NestedFandChiSqTest(lin, cub)
anova(skin3.lm) # seeing that after fitting the linear and quadratic term,
NestedFandChiSqTest(lin, cub)
lin
cub <- lm(Skin ~ Conc + I(Conc^2) + I(Conc^3), data=ratsData)
NestedFandChiSqTest(lin, cub)
anova(skin3.lm) # seeing that after fitting the linear and quadratic term,
NestedFandChiSqTest(lin, cub)
anova(skin2.lm)
NestedFandChiSqTest(lin, quad)
anova(skin3.lm)
NestedFandChiSqTest(lin, cub)
NestedFandChiSqTest(quad, cub)
anova(skin3.lm)
NestedFandChiSqTest(quad, cub)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/FORMULAS.R', echo=TRUE)
anova(skin3.lm)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/FORMULAS.R', echo=TRUE)
anova(skin3.lm)
NestedFTest(quad, cub)
anova(skin2.lm)
NestedFTest(lin, quad)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 5 - Principles of Model Building")
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 5 - Principles of Model Building/lecturedata/")
powerData <- read.table("POWERLOADS.txt", header=TRUE)
head(powerData)
library(ggplot2)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/PLOTTING.R', echo=TRUE)
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/FORMULAS.R', echo=TRUE)
View(powerData)
ggplot(powerData, aes(x=TEMP, y=LOAD)) +
geom_point(shape=19, size=3, color="dodgerblue")
ggplot(powerData, aes(x=TEMP, y=LOAD)) +
geom_point(shape=19, size=3, color="dodgerblue") +
ggtitle("Plot of Temperature vs. Power Load")
power1.lm <- lm(LOAD ~ TEMP, data=powerData)
summary(power1.lm)
attach(powerData)
detach(powerData)
residualFittedPlot(power1.lm)
partialPlot(power1.lm, variableName = "TEMP")
normalQQPlot(power1.lm)
shapiro.test(power1.lm)
shapiro.test(power1.lm$residuals)
power2.lm <- lm(LOAD ~ TEMP + I(TEMP^2), data=powerData)
summary(power2.lm)
library(ggfortify)
autoplot(power2.lm, which=1:2)
anova(power2.lm)
power3.lm <- lm(LOAD ~ TEMP + I(TEMP^2) + I(TEMP^3), data=powerData)
summary(power3.lm)
anova(power3.lm)
autoload(power3.lm, which=1:2)
autoplot(power3.lm, which=1:2)
p1 <- residualFittedPlot(power2.lm)
p2 <- normalQQPlot(power2.lm)
multiplot(p1, p2, ncols=2)
multiplot(p1, p2, cols=2)
multiplot(p1, p2, col=2)
p1
p2
multiplot(p1, p2, cols=2)
summary(power3.lm)
bidsData <- read.table("bids.txt", header=TRUE)
bids1.lm <- lm(Cost ~ State, data=bidsData)
summary(bids1.lm)
View(bidsData)
bidsData$State <- relevel(bidsData$State, ref="Texas")
bids.texas.lm <- lm(Cost ~ State, data=bidsData)
summary(bids.texas.lm)
summary(bids1.lm)
muKansas = bids1.lm$coefficients[[1]]; muKansas
muKentucky = bids1.lm$coefficients[[2]] + muKansas; muKentucky
muTexas = bids1.lm$coefficients[[3]] + muKansas; muTexas
summary(bids.texas.lm)
bids.texas.lm$coefficients[[2]] + bids.texas.lm$coefficients[[1]]
muKentucky = bids1.lm$coefficients[[2]] + muKansas; muKentucky
muTexas = bids1.lm$coefficients[[3]] + muKansas; muTexas
betaCI(bids1.lm)
bids.nointercept.lm <- lm(Cost ~ State -1, data=bidsData)
betaCI(bids.nointercept.lm)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/")
load("data/Exercises and Examples/DIESEL.Rdata")
attach(DIESEL)
# Fitting categorical model, no interaction
diesel.lm <- lm(PERFORM ~ FUEL + BRAND, data=DIESEL)
summary(diesel.lm)
# Fitting with interactions
diesel.interact.lm <- lm(PERFORM ~ FUEL + BRAND + FUEL:BRAND, data=DIESEL)
summary(diesel.interact.lm)
interactionPlot(data=DIESEL, xFactor = "FUEL", traceFactor = "BRAND",
response="PERFORM")
with(DIESEL, interaction.plot(FUEL, BRAND, PERFORM))
interactionPlot(data=DIESEL, xFactor = "FUEL", traceFactor = "BRAND",
response="PERFORM")
with(DIESEL, interaction.plot(FUEL, BRAND, PERFORM))
dieselData <- read.table("DIESEL.txt", header=TRUE)
head(dieselData)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 5 - Principles of Model Building/lecturedata/")
dieselData <- read.table("DIESEL.txt", header=TRUE)
head(dieselData)
with(dieselData, interaction.plot(x.factor = FUEL, trace.factor = BRAND,
response=PERFORM))
interactionPlot(data=dieselData, xFactor = "FUEL", traceFactor = "BRAND",
response="PERFORM")
with(dieselData, interaction.plot(x.factor = BRAND, trace.factor = FUEL,
response=PERFORM))
with(dieselData, interaction.plot(x.factor = FUEL, trace.factor = BRAND,
response=PERFORM))
perform.lm <- lm(PERFORM ~ FUEL + BRAND + FUEL:BRAND, data=dieselData)
summary(perform.lm)
anova(perform.lm)
perform.lm <- lm(PERFORM ~ FUEL:BRAND, data=dieselData)
perform.lm <- lm(PERFORM ~ FUEL:BRAND, data=dieselData)
summary(perform.lm)
summary(perform.lm)
anova(perform.lm)
perform.lm <- lm(PERFORM ~ FUEL*BRAND, data=dieselData)
summary(perform.lm)
anova(perform.lm)
tapply(PERFORM, list(FUEL, BRAND), mean)
with(dieselData, interaction.plot(FUEL, BRAND, PERFORM))
summary(perform.lm)
tapply(PERFORM, list(FUEL, BRAND), mean) # yuo can see they match up
-32.6666667+68.6666667
tapply(PERFORM, list(FUEL, BRAND), sd)
potatData <- read.table("potatoes.txt", header=TRUE)
setwd("/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/Chapter 5 - Principles of Model Building/lecturedata/")
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/PLOTTING.R')
source('/datascience/projects/statisticallyfit/github/R/RStatistics/STAT210 Statistical Modelling and Experimental Design/FORMULAS.R')
library(ggplot2)
potatoData <- read.table("potatoes.txt", header=TRUE)
head(potatoData)
View(potatoData)
with(potatoData, interaction.plot(x.factor = OXYGEN, trace.factor = BAC,
response=ROT))
par(mfrow=c(1,3))
with(potatoData, interaction.plot(x.factor = OXYGEN, trace.factor = BAC,
response=ROT))
with(potatoData, interaction.plot(x.factor = OXYGEN, trace.factor = BAC,
response=ROT))
with(potatoData, interaction.plot(x.factor = BAC, trace.factor = TEMP,
response=ROT))
with(potatoData, interaction.plot(x.factor = OXYGEN, trace.factor = TEMP,
response=ROT))
par(mfrow=c(1,3))
with(potatoData, interaction.plot(x.factor = OXYGEN, trace.factor = BAC,
response=ROT))
with(potatoData, interaction.plot(x.factor = BAC, trace.factor = TEMP,
response=ROT))
with(potatoData, interaction.plot(x.factor = OXYGEN, trace.factor = TEMP,
response=ROT))
install.packages("dae")
library(dae)
interaction.ABC.plot(response=ROT, x.factor = TEMP, groups.factor = BAC,
trace.factor = OXYGEN, data=potatoData)
