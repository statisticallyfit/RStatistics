setwd("/development/projects/statisticallyfit/github/learningmathstat/RStatistics/STAT330 Statistical Learning/ASSIGNMENTS/A4_Project")
library("factoextra")
library("cluster")
library("NbClust")
library("ggplot2")
library("dendextend")
library("corrplot")

## KMEANS CLUSTERING ------------------------------------------------------------------

# clustering factor points
#ggplot(data=pacificData.orig, aes(x=Inshore_fishing_area, y=Coral_reefs, color=factor(kmeans(pacificData, 3, nstart=20)$cluster))) + geom_point(size=3)

# FINCh data
finchData <- read.csv("Finches.csv")
finchData <- na.omit(finchData)
group.gender <- finchData[,2]
finchData <- finchData[,-2] # removing the categorical gender variable


# Scaling data
apply(finchData, 2, mean)
apply(finchData, 2, sd)
# We know the X variables have very different variances , for instance Band has
# variance 1453 while others near 1, so we must standardize 
finchData.scaled <- scale(finchData)

head(finchData)


# Exploratory analysis

# Visualizing relative relationships between observations - creates a heatmap of the 
# relative similarity of each of  observations.
# Values in orange have the highest correlation whereas values in aqua have the lowest 
# correlation.
finch.dist <- get_dist(finchData.scaled, stand = TRUE, method = "pearson")
fviz_dist(finch.dist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))


# Find optimal K (number of kmeans groups to use)
# This method actually uses 30 different indices and then reports
# the frequency of best choice for each.  

# We get best clustering scheme from the different results obtained by varying all 
# combinations of number of clusters, distance measures, and clustering methods.
set.seed(125)
# use min number of clusters = 2, and max = 10
finch.nbclust <- NbClust(finchData.scaled, distance = "euclidean", 
                         min.nc = 2, max.nc = 10, method = "kmeans", index ="all") 
fviz_nbclust(finch.nbclust) + theme_gray()
# by majority rule, choose k = 2 

# Visualize a K-means cluster using K = 2
finch.km <- kmeans(finchData.scaled, 2, nstart = 25)
fviz_cluster(finch.km, data = finchData.scaled, 
             repel=TRUE, 
             star.plot = TRUE, # Add segments from centroids to items
             ellipse.type = "euclid")+theme_gray()

# See which observations are in which cluster
finch.km$cluster
# how many obs per cluster
finch.km$size
# cluster means
finch.km$centers

# tot.withinss = 
# This is the total within-cluster sum of squares, seek to minimize (equation 10.11)
finch.km$tot.withinss

# Conclusion: optimal number of clusters = 2





## HIERARCHICAL CLUSTERING ------------------------------------------------------------------

# Again, use standardized data so that variables are comparable, when measured in different
# scales. 

# Compute dissimilarity matrix: note: data must be scaled since variables are in 
# different units, otherwise dissimilarity measures are severely affected. 
dist.euclid <- dist(finchData.scaled, method = "euclidean")
# Correlation dissimilarity matrix
dist.cor <- get_dist(finchData.scaled, method="pearson")
#dist.corr <- as.dist(1 - cor(t(finchData.scaled)))

# Hierarchical clustering using complete linkage, euclidean distance
finch.complete.euclid.hc <- hclust(dist.euclid, method="complete")
# Hierarchical clustering using complete linkage, correlation distance
finch.complete.cor.hc <- hclust(dist.cor, method="complete")
# Hierarchical clustering using average linkage, euclidean distance
finch.avg.euclid.hc <- hclust(dist.euclid, method="average")
# Hierarchical clustering using average linkage, correlation distance
finch.avg.cor.hc <- hclust(dist.cor, method="average")



# Evaluating the cluster trees:  ++++++++++++++++++++++++++++
# Way 1: 
# compute correlation between cophenetic distances (distances
# of fusion on the dendrogram) and the distance generated by dist() function. If clustering
# was valid, the linking of objects in cluster tree should have strong correlation with
# distances between objects in original distance matrix. 
# Good cor values are 0.75
cor(dist.euclid, cophenetic(finch.complete.euclid.hc))
#  0.6087543
cor(dist.cor, cophenetic(finch.complete.cor.hc))
# [1] 0.5706778
cor(dist.euclid, cophenetic(finch.avg.euclid.hc))
# [1] 0.7131127
cor(dist.cor, cophenetic(finch.avg.cor.hc))
# [1] 0.6581096


# So the correlation with linkage = average, and dist = euclid did the best clustering job. 


# Evaluating the cluster trees:  ++++++++++++++++++++++++++++
# Way 2: 
# Getting optimal k to show the tree cluster groups. 
nb.comp.euc <- NbClust(finchData.scaled, diss=dist.euclid, distance=NULL,
                         min.nc = 2, max.nc = 10, method = "complete", index ="all") 
nb.comp.cor <- NbClust(finchData.scaled, diss=dist.cor, distance=NULL,
                       min.nc = 2, max.nc = 10, method = "complete", index ="all") 
nb.avg.cor <- NbClust(finchData.scaled, diss=dist.cor, distance=NULL,
                       min.nc = 2, max.nc = 10, method = "average", index ="all") 

# Going to show only this one as it scored the best in cophenetic distance. 
nb.avg.euc <- NbClust(finchData.scaled, diss=dist.euclid, distance=NULL,
                      min.nc = 2, max.nc = 10, method = "average", index ="all") 
# All of them used k = 2 as optimal k



# INTERPRET DENDROGRAM: 
# each leaf corresponds to one object. As we move
# up the tree, objects that are similar to each other are combined into branches, which
# are themselves fused at a higher height.
# The height of the fusion, provided on the vertical axis, indicates the 
# (dis)similarity/distance
# between two objects/clusters. The higher the height of the fusion, the less similar the
# objects are.
#  We cannot use
# the proximity of two objects along the horizontal axis as a criteria of their similarity.

fviz_dend(finch.complete.euclid.hc,  k = 2, cex=0.9, color_labels_by_k=TRUE, rect=TRUE, 
          repel=TRUE,ggtheme=theme_gray(), 
          main="Cluser Dendrogram: Complete Linkage, Euclidean Distance")

fviz_dend(finch.complete.cor.hc, k = 2, cex=0.9, color_labels_by_k=TRUE, rect=TRUE, 
          repel=TRUE,ggtheme=theme_gray(), 
          main="Cluser Dendrogram: Complete Linkage, Correlation Distance")

# Showing only this
fviz_dend(finch.avg.euclid.hc, k = 2, cex=0.9, color_labels_by_k=TRUE, rect=TRUE, 
          repel=TRUE,ggtheme=theme_gray(), 
          main="Cluser Dendrogram: Average Linkage, Euclidean Distance")

# See: majority of observations are in cluster 1, contrary to kmeans model
cutree(finch.avg.euclid.hc, k = 2)


fviz_dend(finch.avg.cor.hc, k = 2, cex=0.9, color_labels_by_k=TRUE, rect=TRUE, 
          repel=TRUE,ggtheme=theme_gray(), 
          main="Cluser Dendrogram: Average Linkage, Correlation Distance")


# Compare the dendrograms: 
# Entanglement = measure between 1 (full entanglement) and 0 (no entanglement)
# Lower means good alignment
# The "Unique" nodes (i.e., 
 # what occurs in one of the dendrograms but not the other) are shown with dashed lines.

# These were the two highest models (yielding highest correlation between distance
# matrix and cophenetic distance)
dend.comp.cor <- as.dendrogram(finch.complete.cor.hc)
dend.avg.euclid <- as.dendrogram(finch.avg.euclid.hc)
# list to hold dendrograms
dendList <- dendlist(dend.comp.cor, dend.avg.euclid)

tanglegram(dend.comp.cor, dend.avg.euclid, 
           common_subtrees_color_branches=TRUE, # Color common branches
           main=paste("entanglement = ", round(entanglement(dendList), 3)))


# Another way to compare dendrograms: correlation matrix between a list
dendList <- dendlist("Avg_Cor"=as.dendrogram(finch.avg.cor.hc), 
                     "Avg_Euclid"=as.dendrogram(finch.avg.euclid.hc),
                     "Complete_Cor"=as.dendrogram(finch.complete.cor.hc), 
                     "Complete_Euclid"=as.dendrogram(finch.complete.euclid.hc))
# Values range -1, 1 with 0 meaning the trees are not statistically similar
corMat <- cor.dendlist(dendList, method="cophenetic") 

# Visualize the correlation matrix using corrplot package
corrplot(corMat, "pie", "lower")

